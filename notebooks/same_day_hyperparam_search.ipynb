{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 23:11:17 WARN Utils: Your hostname, scope-vanderbilt resolves to a loopback address: 127.0.1.1; using 10.2.218.69 instead (on interface enp8s0)\n",
      "22/09/15 23:11:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 23:11:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import datetime as dt\n",
    "import importlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import trange, tqdm\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from src import tf_utils, config, data_utils, models, linklevel_utils\n",
    "import logging\n",
    "from itertools import product\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "importlib.reload(tf_utils)\n",
    "importlib.reload(models)\n",
    "from multiprocessing import Process, Queue, cpu_count, Manager\n",
    "\n",
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '40g').master(\"local[26]\")\\\n",
    "    .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "    .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "    .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "    .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "OUTPUT_DIR = os.path.join('../models', 'same_day', 'gridsearch')\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "train_dates = ('2020-01-01', '2021-06-30')\n",
    "val_dates =   ('2021-06-30', '2021-10-31')\n",
    "test_dates =  ('2021-10-31', '2022-04-06')\n",
    "\n",
    "target = 'y_class'\n",
    "\n",
    "num_columns = ['darksky_temperature', 'darksky_humidity', 'darksky_precipitation_probability', 'sched_hdwy']\n",
    "cat_columns = ['month', 'hour', 'day', 'stop_sequence', 'stop_id_original', 'year', target]\n",
    "ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end', 'time_window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'past': 5,\n",
       " 'future': 1,\n",
       " 'offset': 0,\n",
       " 'learning_rate': 0.0001,\n",
       " 'batch_size': 256,\n",
       " 'epochs': 200,\n",
       " 'patience': 10}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAST = 5\n",
    "TIMEWINDOW = 15\n",
    "\n",
    "# STATIC\n",
    "past = PAST\n",
    "future = 1 # Future stops predicted\n",
    "offset = 0\n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "\n",
    "feature_label = config.TARGET_COLUMN_CLASSIFICATION\n",
    "patience = 10\n",
    "\n",
    "hyperparams_dict = {'past': past,\n",
    "                    'future': future,\n",
    "                    'offset': offset,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'epochs': epochs,\n",
    "                    'patience': patience}\n",
    "hyperparams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/home/jptalusan/mta_stationing_problem/models/same_day/gridsearch'\n",
    "CURR_RUN_DIR = f'TW_{TIMEWINDOW}_P_{PAST}'\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_DIR, CURR_RUN_DIR)\n",
    "\n",
    "RANDOM_TEST_TRIPS_PATH = '/home/jptalusan/mta_stationing_problem/models/same_day/evaluation/random_trip_df_5000.pkl'\n",
    "HOLIDAYS_PATH = '/home/jptalusan/mta_stationing_problem/data/others/US Holiday Dates (2004-2021).csv'\n",
    "SCHOOLBREAK_PATH = '/home/jptalusan/mta_stationing_problem/data/others/School Breaks (2019-2022).pkl'\n",
    "\n",
    "Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 23:11:23 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [00:05<00:00, 41.64it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentiles: [(0.0, 6.0), (7.0, 12.0), (13.0, 55.0), (56.0, 75.0), (76.0, 100.0)]\n",
      "Train df:  (7749389, 193)\n",
      "Val df:  (2710767, 193)\n",
      "Test df:  (3563519, 193)\n"
     ]
    }
   ],
   "source": [
    "f = os.path.join('../data', 'processed', 'apc_weather_gtfs.parquet')\n",
    "apcdata = spark.read.load(f)\n",
    "todelete = apcdata.filter('(load < 0) OR (load IS NULL)').select('transit_date','trip_id','overload_id').distinct()\n",
    "todelete=todelete.withColumn('marker',F.lit(1))\n",
    "\n",
    "#joining and whereever the records are not found in sync error table the marker will be null\n",
    "apcdataafternegdelete=apcdata.join(todelete,on=['trip_id','transit_date','overload_id'],how='left').filter('marker is null').drop('marker')\n",
    "apcdataafternegdelete = apcdataafternegdelete.sort(['trip_id', 'overload_id'])\n",
    "\n",
    "get_columns = ['trip_id', 'transit_date', 'arrival_time', \n",
    "                'block_abbr', 'stop_sequence', 'stop_id_original',\n",
    "                'load', \n",
    "                'darksky_temperature', \n",
    "                'darksky_humidity', \n",
    "                'darksky_precipitation_probability', \n",
    "                'route_direction_name', 'route_id',\n",
    "                'dayofweek',  'year', 'month', 'hour',\n",
    "                'sched_hdwy', 'zero_load_at_trip_end']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "\n",
    "apcdataafternegdelete.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "# # filter subset\n",
    "query = f\"\"\"\n",
    "            SELECT {get_str}\n",
    "            FROM apc\n",
    "            \"\"\"\n",
    "apcdataafternegdelete = spark.sql(query)\n",
    "apcdataafternegdelete = apcdataafternegdelete.na.fill(value=0,subset=[\"zero_load_at_trip_end\"])\n",
    "df = apcdataafternegdelete.toPandas()\n",
    "df = df[df.arrival_time.notna()]\n",
    "df = df[df.sched_hdwy.notna()]\n",
    "df = df[df.darksky_temperature.notna()]\n",
    "\n",
    "df['route_id_dir'] = df[\"route_id\"].astype(\"str\") + \"_\" + df[\"route_direction_name\"]\n",
    "df['day'] = df[\"arrival_time\"].dt.day\n",
    "df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "# Adding extra features\n",
    "# Holidays\n",
    "fp = os.path.join('../data', 'others', 'US Holiday Dates (2004-2021).csv')\n",
    "holidays_df = pd.read_csv(fp)\n",
    "holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])\n",
    "holidays_df['is_holiday'] = True\n",
    "df = df.merge(holidays_df[['Date', 'is_holiday']], left_on='transit_date', right_on='Date', how='left')\n",
    "df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "df = df.drop(columns=['Date'])\n",
    "    \n",
    "# School breaks\n",
    "fp = os.path.join('../data', 'others', 'School Breaks (2019-2022).pkl')\n",
    "school_break_df = pd.read_pickle(fp)\n",
    "school_break_df['is_school_break'] = True\n",
    "df = df.merge(school_break_df[['Date', 'is_school_break']], left_on='transit_date', right_on='Date', how='left')\n",
    "df['is_school_break'] = df['is_school_break'].fillna(False)\n",
    "df = df.drop(columns=['Date'])\n",
    "\n",
    "sorted_df = []\n",
    "for ba in tqdm(df.block_abbr.unique()):\n",
    "    ba_df = df[df['block_abbr'] == ba]\n",
    "    end_stop = ba_df.stop_sequence.max()\n",
    "    # Same result as creating a fixed_arrival_time (but faster)\n",
    "    ba_df = ba_df[ba_df.stop_sequence != end_stop].reset_index(drop=True)\n",
    "    sorted_df.append(ba_df)\n",
    "        \n",
    "overall_df = pd.concat(sorted_df)\n",
    "drop_cols = ['route_direction_name', 'route_id']\n",
    "drop_cols = [col for col in drop_cols if col in overall_df.columns]\n",
    "overall_df = overall_df.drop(drop_cols, axis=1)\n",
    "\n",
    "overall_df['minute'] = overall_df['arrival_time'].dt.minute\n",
    "overall_df['minuteByWindow'] = overall_df['minute'] // TIMEWINDOW\n",
    "overall_df['temp'] = overall_df['minuteByWindow'] + (overall_df['hour'] * 60 / TIMEWINDOW)\n",
    "overall_df['time_window'] = np.floor(overall_df['temp']).astype('int')\n",
    "overall_df = overall_df.drop(columns=['minute', 'minuteByWindow', 'temp'])\n",
    "\n",
    "## Aggregate stops by time window\n",
    "# Group by time windows and get the maximum of the aggregate load/class/sched\n",
    "# Get mean of temperature (mostly going to be equal)\n",
    "# TODO: Double check this! \n",
    "overall_df = overall_df.groupby(['transit_date', \n",
    "                                'route_id_dir', \n",
    "                                'stop_id_original',\n",
    "                                'time_window']).agg({\"trip_id\":\"first\",\n",
    "                                                     \"block_abbr\":\"first\",\n",
    "                                                     \"arrival_time\":\"first\",\n",
    "                                                     \"year\":\"first\", \n",
    "                                                     \"month\":\"first\",\n",
    "                                                     \"day\": \"first\",\n",
    "                                                     \"hour\":\"first\",\n",
    "                                                     \"is_holiday\": \"first\",\n",
    "                                                     \"is_school_break\":\"first\",\n",
    "                                                     \"dayofweek\":\"first\",\n",
    "                                                     \"zero_load_at_trip_end\":\"first\",\n",
    "                                                     \"stop_sequence\":\"first\",\n",
    "                                                     \"darksky_temperature\":\"mean\", \n",
    "                                                     \"darksky_humidity\":\"mean\",\n",
    "                                                     \"darksky_precipitation_probability\": \"mean\",\n",
    "                                                     \"sched_hdwy\": \"max\",\n",
    "                                                     \"load\": \"sum\" })\n",
    "overall_df = overall_df.reset_index(level=[0,1,2,3])\n",
    "overall_df = overall_df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "drop_cols = ['arrival_time', 'block_abbr']\n",
    "drop_cols = [col for col in drop_cols if col in overall_df.columns]\n",
    "overall_df = overall_df.drop(drop_cols, axis=1)\n",
    "# checking bins of loads for possible classification problem\n",
    "loads = overall_df[overall_df.load <= config.TARGET_MAX]['load']\n",
    "percentiles = []\n",
    "for cbin in config.CLASS_BINS:\n",
    "    percentile = np.percentile(loads.values, cbin)\n",
    "    percentiles.append(percentile)\n",
    "\n",
    "percentiles = [(percentiles[0], percentiles[1]), (percentiles[1] + 1, percentiles[2]), (percentiles[2] + 1, 55.0), (56.0, 75.0), (76.0, 100.0)]\n",
    "    \n",
    "print(f\"Percentiles: {percentiles}\")\n",
    "overall_df[config.TARGET_COLUMN_CLASSIFICATION] = overall_df['load'].apply(lambda x: data_utils.get_class(x, percentiles))\n",
    "overall_df = overall_df[overall_df[config.TARGET_COLUMN_CLASSIFICATION].notna()]\n",
    "\n",
    "ohe_encoder, label_encoder, num_scaler, train_df, val_df, test_df = linklevel_utils.prepare_linklevel(overall_df, \n",
    "                                                                                                        train_dates=train_dates, \n",
    "                                                                                                        val_dates=val_dates, \n",
    "                                                                                                        test_dates=test_dates,\n",
    "                                                                                                        cat_columns=cat_columns,\n",
    "                                                                                                        num_columns=num_columns,\n",
    "                                                                                                        ohe_columns=ohe_columns,\n",
    "                                                                                                        feature_label='y_class',\n",
    "                                                                                                        time_feature_used='transit_date',\n",
    "                                                                                                        scaler='minmax')\n",
    "\n",
    "drop_cols = ['transit_date', 'load', 'trip_id', 'arrival_time'] + ohe_columns\n",
    "drop_cols = [col for col in drop_cols if col in train_df.columns]\n",
    "train_df = train_df.drop(drop_cols, axis=1)\n",
    "val_df = val_df.drop(drop_cols, axis=1)\n",
    "\n",
    "arrange_cols = [target] + [col for col in train_df.columns if col != target]\n",
    "train_df = train_df[arrange_cols]\n",
    "val_df = val_df[arrange_cols]\n",
    "\n",
    "train_df['y_class'] = train_df.y_class.astype('int')\n",
    "val_df['y_class']   = val_df.y_class.astype('int')\n",
    "test_df['y_class']  = test_df.y_class.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jptalusan/mta_stationing_problem/models/same_day/gridsearch/TW_15_P_5/LL_X_columns.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Saving encoders, scalers and column arrangement\n",
    "fp = os.path.join(OUTPUT_PATH, 'LL_OHE_encoder.joblib')\n",
    "joblib.dump(ohe_encoder, fp)\n",
    "fp = os.path.join(OUTPUT_PATH, 'LL_Label_encoders.joblib')\n",
    "joblib.dump(label_encoder, fp)\n",
    "fp = os.path.join(OUTPUT_PATH, 'LL_Num_scaler.joblib')\n",
    "joblib.dump(num_scaler, fp)\n",
    "fp = os.path.join(OUTPUT_PATH, 'LL_X_columns.joblib')\n",
    "joblib.dump(train_df.columns, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['transit_date', 'load', 'trip_id', 'arrival_time'] + ohe_columns\n",
    "drop_cols = [col for col in drop_cols if col in train_df.columns]\n",
    "train_df = train_df.drop(drop_cols, axis=1)\n",
    "val_df = val_df.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 18:17:53.167923: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-15 18:17:57.534409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11402 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:0b:00.0, compute capability: 6.1\n",
      "2022-09-15 18:18:03.008851: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 11407100608 exceeds 10% of free system memory.\n",
      "2022-09-15 18:18:11.028561: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 11407100608 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Can add shuffle in the future\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def timeseries_dataset_from_dataset(df, feature_slice, label_slice, input_sequence_length, output_sequence_length, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "    ds = dataset.window(input_sequence_length + output_sequence_length, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x).batch(input_sequence_length + output_sequence_length)\n",
    "     \n",
    "    def split_feature_label(x):\n",
    "        return x[:input_sequence_length:, feature_slice], x[input_sequence_length:,label_slice]\n",
    "     \n",
    "    ds = ds.map(split_feature_label)\n",
    "     \n",
    "    return ds.batch(batch_size)\n",
    "\n",
    "label_index = train_df.columns.tolist().index(target)\n",
    "print(label_index)\n",
    "label_slice = slice(label_index, label_index + 1, None) # which column the label/labels are\n",
    "feature_slice = slice(None, None, None) # Which feature columns are included, by default includes all (even label)\n",
    "input_sequence_length = past # number of past information to look at\n",
    "output_sequence_length = future # number of time steps to predict\n",
    "\n",
    "dataset_train = timeseries_dataset_from_dataset(train_df, \n",
    "                                                feature_slice=feature_slice,\n",
    "                                                label_slice=label_slice,\n",
    "                                                input_sequence_length=input_sequence_length, \n",
    "                                                output_sequence_length=output_sequence_length, \n",
    "                                                batch_size=batch_size)\n",
    "\n",
    "# dataset_val = timeseries_dataset_from_dataset(val_df, \n",
    "#                                               feature_slice=feature_slice,\n",
    "#                                               label_slice=label_slice,\n",
    "#                                               input_sequence_length=input_sequence_length, \n",
    "#                                               output_sequence_length=output_sequence_length, \n",
    "#                                               batch_size=batch_size)\n",
    "\n",
    "# dataset_test = timeseries_dataset_from_dataset(test_df,\n",
    "#                                                feature_slice=feature_slice,\n",
    "#                                                label_slice=label_slice,\n",
    "#                                                input_sequence_length=input_sequence_length, \n",
    "#                                                output_sequence_length=output_sequence_length, \n",
    "#                                                batch_size=batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 18:18:16.343459: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 11407100608 exceeds 10% of free system memory.\n",
      "2022-09-15 18:18:19.209348: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 11407100608 exceeds 10% of free system memory.\n",
      "2022-09-15 18:18:26.873499: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  30270/Unknown - 218s 7ms/step - loss: 0.3372 - sparse_categorical_accuracy: 0.8757WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy\n",
      "30272/30272 [==============================] - 218s 7ms/step - loss: 0.3372 - sparse_categorical_accuracy: 0.8757\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_df.y_class.unique())\n",
    "num_classes\n",
    "# define model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "input_shape = (None, None, len(train_df.columns))\n",
    "model.build(input_shape)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "checkpoint_filepath = os.path.join(OUTPUT_PATH, 'CLA_cp-epoch{epoch:02d}-loss{loss:.2f}.ckpt')\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "# fit model\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True), model_checkpoint_callback]\n",
    "history = model.fit(dataset_train, epochs=1, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:01<00:00, 124.51it/s]                              \n"
     ]
    }
   ],
   "source": [
    "f = os.path.join('../data', 'processed', 'apc_weather_gtfs.parquet')\n",
    "apcdata = spark.read.load(f)\n",
    "todelete = apcdata.filter('(load < 0) OR (load IS NULL)').select('transit_date','trip_id','overload_id').distinct()\n",
    "todelete=todelete.withColumn('marker',F.lit(1))\n",
    "\n",
    "#joining and whereever the records are not found in sync error table the marker will be null\n",
    "apcdataafternegdelete=apcdata.join(todelete,on=['trip_id','transit_date','overload_id'],how='left').filter('marker is null').drop('marker')\n",
    "apcdataafternegdelete = apcdataafternegdelete.sort(['trip_id', 'overload_id'])\n",
    "\n",
    "get_columns = ['trip_id', 'transit_date', 'arrival_time', \n",
    "                'block_abbr', 'stop_sequence', 'stop_id_original',\n",
    "                'load', \n",
    "                'darksky_temperature', \n",
    "                'darksky_humidity', \n",
    "                'darksky_precipitation_probability', \n",
    "                'route_direction_name', 'route_id',\n",
    "                'dayofweek',  'year', 'month', 'hour',\n",
    "                'sched_hdwy', 'zero_load_at_trip_end']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "\n",
    "apcdataafternegdelete.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "# # filter subset\n",
    "query = f\"\"\"\n",
    "SELECT {get_str}\n",
    "FROM apc\n",
    "WHERE transit_date >= '{test_dates[0]}' AND transit_date <= '{test_dates[1]}'\n",
    "\"\"\"\n",
    "\n",
    "apcdataafternegdelete = spark.sql(query)\n",
    "apcdataafternegdelete = apcdataafternegdelete.na.fill(value=0,subset=[\"zero_load_at_trip_end\"])\n",
    "df = apcdataafternegdelete.toPandas()\n",
    "df = df[df.arrival_time.notna()]\n",
    "df = df[df.sched_hdwy.notna()]\n",
    "df = df[df.darksky_temperature.notna()]\n",
    "\n",
    "df['route_id_dir'] = df[\"route_id\"].astype(\"str\") + \"_\" + df[\"route_direction_name\"]\n",
    "df['day'] = df[\"arrival_time\"].dt.day\n",
    "df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "# Adding extra features\n",
    "# Holidays\n",
    "fp = os.path.join(HOLIDAYS_PATH)\n",
    "holidays_df = pd.read_csv(fp)\n",
    "holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])\n",
    "holidays_df['is_holiday'] = True\n",
    "df = df.merge(holidays_df[['Date', 'is_holiday']], left_on='transit_date', right_on='Date', how='left')\n",
    "df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "df = df.drop(columns=['Date'])\n",
    "    \n",
    "# School breaks\n",
    "fp = os.path.join(SCHOOLBREAK_PATH)\n",
    "school_break_df = pd.read_pickle(fp)\n",
    "school_break_df['is_school_break'] = True\n",
    "df = df.merge(school_break_df[['Date', 'is_school_break']], left_on='transit_date', right_on='Date', how='left')\n",
    "df['is_school_break'] = df['is_school_break'].fillna(False)\n",
    "df = df.drop(columns=['Date'])\n",
    "\n",
    "sorted_df = []\n",
    "for ba in tqdm(df.block_abbr.unique()):\n",
    "    ba_df = df[df['block_abbr'] == ba]\n",
    "    end_stop = ba_df.stop_sequence.max()\n",
    "    # Same result as creating a fixed_arrival_time (but faster)d\n",
    "    ba_df = ba_df[ba_df.stop_sequence != end_stop].reset_index(drop=True)\n",
    "    sorted_df.append(ba_df)\n",
    "        \n",
    "overall_df = pd.concat(sorted_df)\n",
    "drop_cols = ['route_direction_name', 'route_id']\n",
    "drop_cols = [col for col in drop_cols if col in overall_df.columns]\n",
    "overall_df = overall_df.drop(drop_cols, axis=1)\n",
    "\n",
    "overall_df['minute'] = overall_df['arrival_time'].dt.minute\n",
    "overall_df['minuteByWindow'] = overall_df['minute'] // TIMEWINDOW\n",
    "overall_df['temp'] = overall_df['minuteByWindow'] + (overall_df['hour'] * 60 / TIMEWINDOW)\n",
    "overall_df['time_window'] = np.floor(overall_df['temp']).astype('int')\n",
    "overall_df = overall_df.drop(columns=['minute', 'minuteByWindow', 'temp'])\n",
    "overall_df = overall_df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_linklevel(test_df, ohe_encoder, num_scaler, label_encoders,\n",
    "                        cat_columns=None, num_columns=None, ohe_columns=None, feature_label='load'):\n",
    "    test_df[ohe_encoder.get_feature_names_out()] = ohe_encoder.transform(test_df[ohe_columns]).toarray()\n",
    "    for col in [col for col in cat_columns if col != feature_label]:\n",
    "        encoder = label_encoders[col]\n",
    "        test_df[col] = encoder.transform(test_df[col])\n",
    "    test_df[num_columns] = num_scaler.transform(test_df[num_columns])\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "test_df = deepcopy(overall_df)\n",
    "\n",
    "columns = joblib.load(f'{OUTPUT_PATH}/LL_X_columns.joblib')\n",
    "label_encoders = joblib.load(f'{OUTPUT_PATH}/LL_Label_encoders.joblib')\n",
    "ohe_encoder = joblib.load(f'{OUTPUT_PATH}/LL_OHE_encoder.joblib')\n",
    "num_scaler = joblib.load(f'{OUTPUT_PATH}/LL_Num_scaler.joblib')\n",
    "\n",
    "test_df = prepare_test_linklevel(test_df, \n",
    "                                ohe_encoder, num_scaler, label_encoders,\n",
    "                                cat_columns=cat_columns,\n",
    "                                num_columns=num_columns,\n",
    "                                ohe_columns=ohe_columns,\n",
    "                                feature_label='y_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revere_transform(df, label_encoders, ohe_encoder):\n",
    "    \n",
    "    for col in cat_columns:\n",
    "        if col == 'y_class':\n",
    "            continue\n",
    "        df[col] = label_encoders[col].inverse_transform(df[col])\n",
    "        \n",
    "    df[ohe_columns] = ohe_encoder.inverse_transform(df.filter(regex='dayofweek_|route_id_dir_|is_holiday_|is_school_break_|zero_load_at_trip_end_|time_window_'))\n",
    "    df = df.drop(columns=df.filter(regex='dayofweek_|route_id_dir_|is_holiday_|is_school_|zero_load_|time_window_').columns, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_lstm_predictions(input_df, model, future):\n",
    "    predictions = []\n",
    "    for f in range(future):\n",
    "        pred = model.predict(input_df.to_numpy().reshape(1, *input_df.shape))\n",
    "        y_pred = np.argmax(pred)\n",
    "        predictions.append(y_pred)\n",
    "        last_row = input_df.iloc[[-1]]\n",
    "        last_row['y_class'] = y_pred\n",
    "        last_row['stop_sequence'] = last_row['stop_sequence'] + 1\n",
    "        input_df = pd.concat([input_df[1:], last_row])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['y_class'] = test_df['load'].apply(lambda x: data_utils.get_class(x, percentiles))\n",
    "test_df = test_df[test_df['y_class'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 14/1000 [00:18<21:27,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [27:09<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "num_features = len(train_df.columns)\n",
    "simple_lstm = linklevel_utils.setup_simple_lstm_generator(num_features, len(test_df.y_class.unique()))\n",
    "# Load model\n",
    "latest = tf.train.latest_checkpoint(OUTPUT_PATH)\n",
    "\n",
    "simple_lstm.load_weights(latest)\n",
    "\n",
    "# Load random trips for evaluation\n",
    "fp = os.path.join(RANDOM_TEST_TRIPS_PATH)\n",
    "random_trip_df = pd.read_pickle(fp)\n",
    "results = []\n",
    "for i in tqdm(range(len(random_trip_df[0:1000]))):\n",
    "    trip_df = test_df.merge(random_trip_df.iloc[[i]], on=['transit_date', 'trip_id', 'route_id_dir'])\n",
    "    \n",
    "    inverse_trip_df = deepcopy(trip_df)\n",
    "    inverse_trip_df = revere_transform(inverse_trip_df, label_encoders, ohe_encoder)\n",
    "    drop_cols = ['transit_date', 'load', 'trip_id', 'arrival_time'] + ohe_columns\n",
    "    drop_cols = [col for col in drop_cols if col in trip_df.columns]\n",
    "    trip_df = trip_df.drop(drop_cols, axis=1)\n",
    "\n",
    "    if len(trip_df) == 0:\n",
    "        continue\n",
    "    trip_df = trip_df[train_df.columns]\n",
    "    \n",
    "    future = len(trip_df) - PAST\n",
    "    past_df = trip_df.iloc[0:PAST]\n",
    "    \n",
    "    inverse_trip_df['y_pred'] = -1\n",
    "\n",
    "    y_true = trip_df.iloc[PAST:].y_class.tolist()\n",
    "    y_pred = generate_simple_lstm_predictions(past_df, simple_lstm, future)\n",
    "    inverse_trip_df.loc[trip_df.iloc[PAST:].index, 'y_pred'] = y_pred\n",
    "    results.append(inverse_trip_df)\n",
    "\n",
    "results = pd.concat(results)\n",
    "fp = os.path.join(OUTPUT_PATH, 'results_5000_df.pkl')\n",
    "results.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>transit_date</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>block_abbr</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_id_original</th>\n",
       "      <th>load</th>\n",
       "      <th>darksky_temperature</th>\n",
       "      <th>darksky_humidity</th>\n",
       "      <th>darksky_precipitation_probability</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>sched_hdwy</th>\n",
       "      <th>route_id_dir</th>\n",
       "      <th>day</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>time_window</th>\n",
       "      <th>y_class</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259477</td>\n",
       "      <td>2022-01-15</td>\n",
       "      <td>2022-01-15 15:01:38</td>\n",
       "      <td>1701</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC5_11</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.415964</td>\n",
       "      <td>0.73125</td>\n",
       "      <td>0.006</td>\n",
       "      <td>7</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.031283</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>60</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259477</td>\n",
       "      <td>2022-01-15</td>\n",
       "      <td>2022-01-15 15:21:12</td>\n",
       "      <td>1701</td>\n",
       "      <td>2</td>\n",
       "      <td>CHA7AWN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.415964</td>\n",
       "      <td>0.73125</td>\n",
       "      <td>0.006</td>\n",
       "      <td>7</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.031283</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259477</td>\n",
       "      <td>2022-01-15</td>\n",
       "      <td>2022-01-15 15:23:54</td>\n",
       "      <td>1701</td>\n",
       "      <td>3</td>\n",
       "      <td>11ACHASF</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.415964</td>\n",
       "      <td>0.73125</td>\n",
       "      <td>0.006</td>\n",
       "      <td>7</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.031283</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>259477</td>\n",
       "      <td>2022-01-15</td>\n",
       "      <td>2022-01-15 15:24:48</td>\n",
       "      <td>1701</td>\n",
       "      <td>4</td>\n",
       "      <td>11ACHUSF</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.415964</td>\n",
       "      <td>0.73125</td>\n",
       "      <td>0.006</td>\n",
       "      <td>7</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.031283</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259477</td>\n",
       "      <td>2022-01-15</td>\n",
       "      <td>2022-01-15 15:25:28</td>\n",
       "      <td>1701</td>\n",
       "      <td>5</td>\n",
       "      <td>11APORSF</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.415964</td>\n",
       "      <td>0.73125</td>\n",
       "      <td>0.006</td>\n",
       "      <td>7</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.031283</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>260615</td>\n",
       "      <td>2022-03-27</td>\n",
       "      <td>2022-03-27 12:26:24</td>\n",
       "      <td>2311</td>\n",
       "      <td>62</td>\n",
       "      <td>BRICHEEM</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.456821</td>\n",
       "      <td>0.14875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.018770</td>\n",
       "      <td>23_FROM DOWNTOWN</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>260615</td>\n",
       "      <td>2022-03-27</td>\n",
       "      <td>2022-03-27 12:26:36</td>\n",
       "      <td>2311</td>\n",
       "      <td>63</td>\n",
       "      <td>BRICHEEN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.456821</td>\n",
       "      <td>0.14875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.018770</td>\n",
       "      <td>23_FROM DOWNTOWN</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>260615</td>\n",
       "      <td>2022-03-27</td>\n",
       "      <td>2022-03-27 12:26:56</td>\n",
       "      <td>2311</td>\n",
       "      <td>64</td>\n",
       "      <td>CHEWOOSN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.456821</td>\n",
       "      <td>0.14875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.018770</td>\n",
       "      <td>23_FROM DOWNTOWN</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>260615</td>\n",
       "      <td>2022-03-27</td>\n",
       "      <td>2022-03-27 12:27:10</td>\n",
       "      <td>2311</td>\n",
       "      <td>65</td>\n",
       "      <td>CHERAISN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.456821</td>\n",
       "      <td>0.14875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.018770</td>\n",
       "      <td>23_FROM DOWNTOWN</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>260615</td>\n",
       "      <td>2022-03-27</td>\n",
       "      <td>2022-03-27 12:27:26</td>\n",
       "      <td>2311</td>\n",
       "      <td>66</td>\n",
       "      <td>EAGDOVSN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.456821</td>\n",
       "      <td>0.14875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.018770</td>\n",
       "      <td>23_FROM DOWNTOWN</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36476 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_id transit_date        arrival_time  block_abbr  stop_sequence  \\\n",
       "0   259477   2022-01-15 2022-01-15 15:01:38        1701              1   \n",
       "1   259477   2022-01-15 2022-01-15 15:21:12        1701              2   \n",
       "2   259477   2022-01-15 2022-01-15 15:23:54        1701              3   \n",
       "3   259477   2022-01-15 2022-01-15 15:24:48        1701              4   \n",
       "4   259477   2022-01-15 2022-01-15 15:25:28        1701              5   \n",
       "..     ...          ...                 ...         ...            ...   \n",
       "58  260615   2022-03-27 2022-03-27 12:26:24        2311             62   \n",
       "59  260615   2022-03-27 2022-03-27 12:26:36        2311             63   \n",
       "60  260615   2022-03-27 2022-03-27 12:26:56        2311             64   \n",
       "61  260615   2022-03-27 2022-03-27 12:27:10        2311             65   \n",
       "62  260615   2022-03-27 2022-03-27 12:27:26        2311             66   \n",
       "\n",
       "   stop_id_original  load  darksky_temperature  darksky_humidity  \\\n",
       "0           MCC5_11  13.0             0.415964           0.73125   \n",
       "1           CHA7AWN  13.0             0.415964           0.73125   \n",
       "2          11ACHASF  14.0             0.415964           0.73125   \n",
       "3          11ACHUSF  14.0             0.415964           0.73125   \n",
       "4          11APORSF  14.0             0.415964           0.73125   \n",
       "..              ...   ...                  ...               ...   \n",
       "58         BRICHEEM   3.0             0.456821           0.14875   \n",
       "59         BRICHEEN   3.0             0.456821           0.14875   \n",
       "60         CHEWOOSN   3.0             0.456821           0.14875   \n",
       "61         CHERAISN   3.0             0.456821           0.14875   \n",
       "62         EAGDOVSN   3.0             0.456821           0.14875   \n",
       "\n",
       "    darksky_precipitation_probability dayofweek  year  month  hour  \\\n",
       "0                               0.006         7  2022      1  15.0   \n",
       "1                               0.006         7  2022      1  15.0   \n",
       "2                               0.006         7  2022      1  15.0   \n",
       "3                               0.006         7  2022      1  15.0   \n",
       "4                               0.006         7  2022      1  15.0   \n",
       "..                                ...       ...   ...    ...   ...   \n",
       "58                              0.000         1  2022      3  12.0   \n",
       "59                              0.000         1  2022      3  12.0   \n",
       "60                              0.000         1  2022      3  12.0   \n",
       "61                              0.000         1  2022      3  12.0   \n",
       "62                              0.000         1  2022      3  12.0   \n",
       "\n",
       "    sched_hdwy      route_id_dir  day is_holiday time_window  y_class  y_pred  \n",
       "0     0.031283  17_FROM DOWNTOWN   15      False          60      2.0      -1  \n",
       "1     0.031283  17_FROM DOWNTOWN   15      False          61      2.0      -1  \n",
       "2     0.031283  17_FROM DOWNTOWN   15      False          61      2.0      -1  \n",
       "3     0.031283  17_FROM DOWNTOWN   15      False          61      2.0      -1  \n",
       "4     0.031283  17_FROM DOWNTOWN   15      False          61      2.0      -1  \n",
       "..         ...               ...  ...        ...         ...      ...     ...  \n",
       "58    0.018770  23_FROM DOWNTOWN   27      False          49      0.0       0  \n",
       "59    0.018770  23_FROM DOWNTOWN   27      False          49      0.0       0  \n",
       "60    0.018770  23_FROM DOWNTOWN   27      False          49      0.0       0  \n",
       "61    0.018770  23_FROM DOWNTOWN   27      False          49      0.0       0  \n",
       "62    0.018770  23_FROM DOWNTOWN   27      False          49      0.0       0  \n",
       "\n",
       "[36476 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
