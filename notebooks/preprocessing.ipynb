{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfbfc402-614b-4488-9f8c-29c0967bac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import importlib\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import TimestampType, DateType,DoubleType,FloatType,IntegerType,StringType,StructType,ArrayType,StructField\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from scipy.stats import zscore\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "import numpy as np\n",
    "import gtfs_kit as gk\n",
    "import time\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630557db-e992-426d-b76f-0d415e7e8215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/04 02:43:03 WARN Utils: Your hostname, scope-vanderbilt resolves to a loopback address: 127.0.1.1; using 10.2.218.69 instead (on interface enp8s0)\n",
      "22/08/04 02:43:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/04 02:43:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/04 02:43:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/04 02:43:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '40g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '20g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41475fa5-cc7b-4a5b-95d1-8148ed4b531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate load\n",
    "def get_derived_load(stops_in):\n",
    "    stops = stops_in.sort_values(by=['scheduled_time'])\n",
    "    stops = stops.iloc[1:len(stops)-1]\n",
    "    last_load = stops.iloc[0]['load']\n",
    "    derived_load = [last_load]\n",
    "    for k in range(1, len(stops)):\n",
    "        cur_stop = stops.iloc[k]\n",
    "        cur_load = last_load + cur_stop['ons'] - cur_stop['offs']\n",
    "        derived_load.append(cur_load)\n",
    "        last_load = cur_load\n",
    "    stops['derived_load'] = derived_load\n",
    "    return stops\n",
    "\n",
    "def timestr_to_seconds(timestr):\n",
    "    temp = [int(x) for x in timestr.split(\":\")]\n",
    "    hour, minute, second = temp[0], temp[1], temp[2]\n",
    "    return second + minute*60 + hour*3600\n",
    "\n",
    "def timestr_to_hour(timestr):\n",
    "    temp = [int(x) for x in timestr.split(\":\")]\n",
    "    hour, minute, second = temp[0], temp[1], temp[2]\n",
    "    return hour\n",
    "\n",
    "def get_days_of_week(week_arr):\n",
    "    daysofweek = []\n",
    "    if week_arr[0] == 1:\n",
    "        daysofweek.append(1)\n",
    "    if week_arr[1] == 1:\n",
    "        daysofweek.append(2)\n",
    "    if week_arr[2] == 1:\n",
    "        daysofweek.append(3)\n",
    "    if week_arr[3] == 1:\n",
    "        daysofweek.append(4)\n",
    "    if week_arr[4] == 1:\n",
    "        daysofweek.append(5)\n",
    "    if week_arr[5] == 1:\n",
    "        daysofweek.append(6)\n",
    "    if week_arr[6] == 1:\n",
    "        daysofweek.append(7)\n",
    "    return daysofweek\n",
    "\n",
    "def seconds_to_timestr(seconds, format='%H:%M:%S'):\n",
    "    return time.strftime(format, time.gmtime(seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09801933-0591-4fe6-add4-a39926eb9c8c",
   "metadata": {},
   "source": [
    "# APC data\n",
    "* Don't try to load the entire dataset to pandas unless you are on a device with ~100GB RAM.\n",
    "* You can select columns and limit the dates just to see the data:\n",
    "    * `apcdata.columns`: to see column names\n",
    "    * To select certain columns/filter dates\n",
    "    ```\n",
    "        query=f\"\"\"SELECT transit_date, trip_id, vehicle_id,\n",
    "                    block_abbr, arrival_time, \n",
    "                    scheduled_time, departure_time,\n",
    "                    block_stop_order, load, load_factor,\n",
    "                    map_latitude, map_longitude, offs, ons, overload_id,\n",
    "                    pattern_num, route_direction_name, route_id,\n",
    "                    stop_id, stop_name, stop_sequence, year, month,\n",
    "                    day, hour, dayofweek\n",
    "                FROM apc\n",
    "                WHERE (transit_date >= '{date_range[0]} 00:00:00') AND (transit_date < '{date_range[1]} 00:00:00')\"\"\"\n",
    "    ```\n",
    "* Column information: Blanks mean they are not used at all (so far)\n",
    "    * `transit_date`: Trip date %Y-%m-%d\n",
    "    * `trip_id`: Trip `\n",
    "    * `vehicle_id`: Vehicle id, one vehicle can travel many blocks and trip\n",
    "    * `block_abbr`: (str) Block`\n",
    "    * `activation_date`:\n",
    "    * `activation_date_str`: %Y-%m-%d\n",
    "    * `arrival_time`: (datetime) Time the bus arrived at the stop\n",
    "    * `arrival_time_str`: (str) %Y-%m-%d\n",
    "    * `block_stop_order`:(int) Order of blocks\n",
    "    * `deactivation_date`:\n",
    "    * `deactivation_date_str`:\n",
    "    * `departure_time`: (datetime) Time the bus left the stop\n",
    "    * `departure_time_str`: (str) %Y-%m-%d\n",
    "    * `load`: (int) Total passengers in the bus (ons - offs)\n",
    "    * `load_factor`: (float)\n",
    "    * `map_latitude`: (float)\n",
    "    * `map_longitude`: (float)\n",
    "    * `offs`: (int) People alighting\n",
    "    * `ons`: (int) People boarding\n",
    "    * `overload_id`: (int) 0 or 1, Whether the current vehicle was used as an overload bus\n",
    "    * `pattern_num`:\n",
    "    * `route_direction_name`: (str) Whether it goes TO or FROM Downtown\n",
    "    * `route_id`: (int)\n",
    "    * `scheduled_time`: (datetime) Time the bus was scheduled to arrive at the stop\n",
    "    * `scheduled_time_str`: (str)  %Y-%m-%d\n",
    "    * `source_pattern_id`:\n",
    "    * `stop_id`: (str) Stop name abbreviation\n",
    "    * `stop_id_list`: (list: str) Stop names (abbr) in order\n",
    "    * `stop_id_original`: (str) Stop name, longer than stop_id but still shortened\n",
    "    * `stop_name`: (str) complete stop name\n",
    "    * `stop_sequence`: (int) Number of the stop in the stop sequence\n",
    "    * `stop_sequence_list`: (list: int)\n",
    "    * `transit_date_str`: (str) %m/%d/%y (08/01/21)\n",
    "    * `update_date`:\n",
    "    * `vehicle_capacity`: (int) Vehicle capacity, although not too reliable as many vehicles have NaN values\n",
    "    * `zero_load_at_trip_end`:\n",
    "    * `year`: (int)\n",
    "    * `month`: (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148a849-5302-40a5-a0ea-ec965641ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the APC data\n",
    "filepath = os.path.join(\"../data\", \"apc\", \"cleaned-wego-daily.apc.parquet\")\n",
    "apcdata = spark.read.load(filepath)\n",
    "\n",
    "# add day and hour of day\n",
    "apcdata = apcdata.withColumn('day', F.dayofmonth(apcdata.transit_date))\n",
    "apcdata = apcdata.withColumn('hour', F.hour(apcdata.arrival_time))\n",
    "apcdata = apcdata.withColumn('dayofweek', F.dayofweek(apcdata.transit_date)) # 1=Sunday, 2=Monday ... 7=Saturday\n",
    "apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM apc\n",
    "\"\"\"\n",
    "apcdata = spark.sql(query)\n",
    "\n",
    "# remove bad trips\n",
    "todelete = apcdata.filter('(ons IS NULL) OR (offs IS NULL) OR (load IS NULL)').select('transit_date','trip_id','overload_id').distinct()\n",
    "todelete=todelete.withColumn('marker',F.lit(1))\n",
    "apcdata=apcdata.join(todelete,on=['transit_date', 'trip_id', 'overload_id'], how='left').filter('marker is null').drop('marker')\n",
    "\n",
    "# remove trips with less then 5 stops\n",
    "todelete = apcdata.groupby('transit_date', 'trip_id', 'overload_id').count().filter(\"count < 4\")\n",
    "todelete=todelete.withColumn('marker',F.lit(1))\n",
    "apcdata=apcdata.join(todelete,on=['transit_date', 'trip_id', 'overload_id'], how='left').filter('marker is null').drop('marker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e08b3b-bd65-41ff-b597-74aaf6d69c65",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Weather - darksky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f662763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>apparent_temperature</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>humidity</th>\n",
       "      <th>icon</th>\n",
       "      <th>nearest_storm_distance</th>\n",
       "      <th>ozone</th>\n",
       "      <th>precipitation_intensity</th>\n",
       "      <th>precipitation_probability</th>\n",
       "      <th>pressure</th>\n",
       "      <th>summary</th>\n",
       "      <th>temperature</th>\n",
       "      <th>time</th>\n",
       "      <th>uv_index</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_bearing</th>\n",
       "      <th>wind_gust</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>darksky</td>\n",
       "      <td>75.99</td>\n",
       "      <td>0.96</td>\n",
       "      <td>58.35</td>\n",
       "      <td>0.54</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>0</td>\n",
       "      <td>328.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.7</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>75.99</td>\n",
       "      <td>1621367859</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>155</td>\n",
       "      <td>22.15</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    agency  apparent_temperature  cloud_cover  dew_point  humidity    icon  \\\n",
       "0  darksky                 75.99         0.96      58.35      0.54  cloudy   \n",
       "\n",
       "   nearest_storm_distance  ozone  precipitation_intensity  \\\n",
       "0                       0  328.1                      0.0   \n",
       "\n",
       "   precipitation_probability  pressure   summary  temperature        time  \\\n",
       "0                        0.0    1020.7  Overcast        75.99  1621367859   \n",
       "\n",
       "   uv_index  visibility  wind_bearing  wind_gust  wind_speed  \n",
       "0         4        10.0           155      22.15        11.4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = os.path.join(\"../data\", \"weather\", \"darksky_nashville_20220406.csv\")\n",
    "darksky = pd.read_csv(filepath)\n",
    "darksky.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385a6d22-1d7e-4573-b9cd-0a4ed23967d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"../data\", \"weather\", \"darksky_nashville_20220406.csv\")\n",
    "darksky = pd.read_csv(filepath)\n",
    "# GMT-5\n",
    "darksky['datetime'] = darksky['time'] - 18000\n",
    "darksky['datetime'] = pd.to_datetime(darksky['datetime'], infer_datetime_format=True, unit='s')\n",
    "darksky = darksky.set_index(darksky['datetime'])\n",
    "# darksky = darksky.sort_index().loc[date_range[0]:date_range[1]]\n",
    "darksky['year'] = darksky['datetime'].dt.year\n",
    "darksky['month'] = darksky['datetime'].dt.month\n",
    "darksky['day'] = darksky['datetime'].dt.day\n",
    "darksky['hour'] = darksky['datetime'].dt.hour\n",
    "val_cols= ['temperature', 'humidity', 'nearest_storm_distance', 'precipitation_intensity', 'precipitation_probability', 'pressure', 'wind_gust', 'wind_speed']\n",
    "join_cols = ['year', 'month', 'day', 'hour']\n",
    "darksky = darksky[val_cols+join_cols]\n",
    "renamed_cols = {k: f\"darksky_{k}\" for k in val_cols}\n",
    "darksky = darksky.rename(columns=renamed_cols)\n",
    "darksky = darksky.groupby(['year', 'month', 'day', 'hour']).mean().reset_index()\n",
    "darksky=spark.createDataFrame(darksky)\n",
    "darksky.createOrReplaceTempView(\"darksky\")\n",
    "\n",
    "# join apc and darksky\n",
    "apcdata = apcdata.join(darksky,on=['year', 'month', 'day', 'hour'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab03504-140d-474c-a92c-8e8aa78550bb",
   "metadata": {},
   "source": [
    "# Weather - weatherbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9756aa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------\n",
      " station_id               | 720259-63844        \n",
      " start_date_st            | 2016-02-05          \n",
      " end_date_st              | 2016-02-12          \n",
      " timestamp_local          | 2010-01-01 00:00:00 \n",
      " rh                       | 75.0                \n",
      " wind_spd                 | 2.1                 \n",
      " timestamp_utc            | 2010-01-01 05:00:00 \n",
      " pod                      | n                   \n",
      " slp                      | 1019.0              \n",
      " app_temp                 | 4.4                 \n",
      " elev_angle               | -75.42              \n",
      " solar_rad                | 0.0                 \n",
      " pres                     | 935.0005            \n",
      " h_angle                  | null                \n",
      " dewpt                    | 1.0                 \n",
      " snow                     | 0.0                 \n",
      " uv                       | 0.0                 \n",
      " azimuth                  | 324.18              \n",
      " wind_dir                 | 350.0               \n",
      " ghi                      | 0.0                 \n",
      " dhi                      | 0.0                 \n",
      " vis                      | 16.0                \n",
      " dni                      | 0.0                 \n",
      " datetime                 | 2010-01-01:05       \n",
      " temp                     | 6.0                 \n",
      " precip                   | 0.0                 \n",
      " clouds                   | 100.0               \n",
      " ts                       | 1.262322E9          \n",
      " weather.icon             | c04n                \n",
      " weather.code             | 804.0               \n",
      " weather.description      | Overcast clouds     \n",
      " gps_coordinate_latitude  | 35.223              \n",
      " gps_coordinate_longitude | -83.419             \n",
      " spatial_id               | Franklin            \n",
      " month                    | 1.0                 \n",
      " year                     | 2010.0              \n",
      " days                     | 2010-01-01          \n",
      " __index_level_0__        | 3912654             \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filepath = os.path.join(\"../data\", \"weather\", \"weatherbit_weather_2010_2022.parquet\")\n",
    "weatherbit = spark.read.load(filepath)\n",
    "weatherbit.show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658c258-44a6-43eb-847d-d8cb647f61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weatherbit\n",
    "filepath = os.path.join(\"../data\", \"weather\", \"weatherbit_weather_2010_2022.parquet\")\n",
    "weatherbit = spark.read.load(filepath)\n",
    "\n",
    "weatherbit = weatherbit.filter(\"(spatial_id = 'Berry Hill') OR (spatial_id = 'Belle Meade')\")\n",
    "weatherbit.createOrReplaceTempView(\"weatherbit\")\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM weatherbit\n",
    "\"\"\"\n",
    "# WHERE (timestamp_local >= '{date_range[0]} 23:00:00') AND (timestamp_local < '{date_range[1]} 00:00:00')\n",
    "weatherbit = spark.sql(query)\n",
    "\n",
    "weatherbit = weatherbit.withColumn('year', F.year(weatherbit.timestamp_local))\n",
    "weatherbit = weatherbit.withColumn('month', F.month(weatherbit.timestamp_local))\n",
    "weatherbit = weatherbit.withColumn('day', F.dayofmonth(weatherbit.timestamp_local))\n",
    "weatherbit = weatherbit.withColumn('hour', F.hour(weatherbit.timestamp_local))\n",
    "weatherbit = weatherbit.select('year', 'month', 'day', 'hour', 'rh', 'wind_spd', 'slp', 'app_temp', 'temp', 'snow', 'precip')\n",
    "weatherbit = weatherbit.groupBy('year', 'month', 'day', 'hour').agg(F.mean('rh').alias('weatherbit_rh'), \\\n",
    "                                                                    F.mean('wind_spd').alias('weatherbit_wind_spd'), \\\n",
    "                                                                    F.mean('app_temp').alias('weatherbit_app_temp'), \\\n",
    "                                                                    F.mean('temp').alias('weatherbit_temp'), \\\n",
    "                                                                    F.mean('snow').alias('weatherbit_snow'), \\\n",
    "                                                                    F.mean('precip').alias('weatherbit_precip')\n",
    "                                                                   )\n",
    "weatherbit = weatherbit.sort(['year', 'month', 'day', 'hour'])\n",
    "\n",
    "# join apc and weatherbit\n",
    "apcdata=apcdata.join(weatherbit,on=['year', 'month', 'day', 'hour'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2a7c5-e945-4526-ae3b-962b88e7b0ff",
   "metadata": {},
   "source": [
    "# Join with GTFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9794dc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------\n",
      " trip_id               | 132152                         \n",
      " arrival_time          | 17:10:00                       \n",
      " bikes_allowed         | 0                              \n",
      " block_id              | b_14613                        \n",
      " departure_time        | 17:10:00                       \n",
      " direction_id          | 0                              \n",
      " drop_off_type         | null                           \n",
      " gtfs_file             | 30-March-2017.zip              \n",
      " location_type         | 0.0                            \n",
      " parent_station        | null                           \n",
      " pickup_type           | 1.0                            \n",
      " route_id              | 19                             \n",
      " route_long_name       | HERMAN                         \n",
      " service_id            | 3_merged_136582                \n",
      " shape_dist_traveled   | 9.0812                         \n",
      " shape_id              | 10625                          \n",
      " stop_code             | MCC5_8                         \n",
      " stop_desc             | UNNAMED & 5TH AVE N            \n",
      " stop_headsign         | null                           \n",
      " stop_id               | MCC5_8                         \n",
      " stop_lat              | 36.166768                      \n",
      " stop_lon              | -86.781424                     \n",
      " stop_name             | MUSIC CITY CENTRAL 5TH - BAY 8 \n",
      " stop_sequence         | 35                             \n",
      " stop_timezone         | null                           \n",
      " stop_url              | null                           \n",
      " timepoint             | null                           \n",
      " trip_headsign         | DOWNTOWN                       \n",
      " trip_short_name       | null                           \n",
      " wheelchair_accessible | 0                              \n",
      " wheelchair_boarding   | 0.0                            \n",
      " zone_id               | null                           \n",
      " original_trip_id      | null                           \n",
      " platform_code         | null                           \n",
      " start_date            | 2017-04-02 00:00:00            \n",
      " end_date              | 2017-09-24 00:00:00            \n",
      " monday                | 0                              \n",
      " tuesday               | 0                              \n",
      " wednesday             | 0                              \n",
      " thursday              | 0                              \n",
      " friday                | 0                              \n",
      " saturday              | 0                              \n",
      " sunday                | 1                              \n",
      " date                  | 2017-03-30 00:00:00            \n",
      " __index_level_0__     | 11410071                       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filepath = os.path.join(\"../data\", \"gtfs\", \"alltrips_mta_wego.parquet\")\n",
    "alltrips = spark.read.load(filepath)\n",
    "alltrips.show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02f90b-73bf-4f15-9686-1f6b700cd2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(os.getcwd(), \"data\", \"static_gtfs\", \"alltrips_mta_wego.parquet\")\n",
    "alltrips = spark.read.load(filepath)\n",
    "\n",
    "# add gtfs_file, gtfs_shape_id, gtfs_route_id, gtfs_direction_id, gtfs_start_date, gtfs_end_date, gtfs_date\n",
    "gtfstrips = alltrips.select('trip_id','date','gtfs_file', 'shape_id', 'route_id', 'direction_id', 'start_date', 'end_date').distinct()\n",
    "gtfstrips = gtfstrips.withColumnRenamed('shape_id', 'gtfs_shape_id')\\\n",
    "                     .withColumnRenamed('route_id', 'gtfs_route_id')\\\n",
    "                     .withColumnRenamed('direction_id', 'gtfs_direction_id')\\\n",
    "                     .withColumnRenamed('start_date','gtfs_start_date')\\\n",
    "                     .withColumnRenamed('end_date','gtfs_end_date')\n",
    "\n",
    "# Some GTFS are outdated?, add transit_date, and trip_id\n",
    "rantrips = apcdata.select('transit_date','trip_id').distinct().join(gtfstrips, on='trip_id', how='left').filter('transit_date >= date')\n",
    "rantrips_best_gtfs_file = rantrips.groupby('transit_date','trip_id').agg(F.max('date').alias('date'))\n",
    "# Inner assures no NaN\n",
    "rantrips = rantrips.join(rantrips_best_gtfs_file, on=['transit_date','trip_id','date'], how='inner').withColumnRenamed('date', 'gtfs_date')\n",
    "# Essentilly rantrips is just the GTFS data with transit_id and trip_id (matched from the apcdata)\n",
    "apcdata = apcdata.join(rantrips,on=['transit_date','trip_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee0dfa-1647-4cce-a418-2f058889fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scheduled number of vehicles on route at the given hour\n",
    "\n",
    "alltrips = alltrips.withColumnRenamed('route_id', 'gtfs_route_id')\\\n",
    "                   .withColumnRenamed('date', 'gtfs_date')\\\n",
    "                   .withColumnRenamed('direction_id', 'gtfs_direction_id')\n",
    "\n",
    "timestrToSecondsUDF = F.udf(lambda x: timestr_to_seconds(x), IntegerType())\n",
    "alltrips = alltrips.withColumn(\"time_seconds\", timestrToSecondsUDF(F.col('arrival_time')))\n",
    "\n",
    "timestrToHourUDF = F.udf(lambda x: timestr_to_hour(x), IntegerType())\n",
    "alltrips = alltrips.withColumn(\"hour\", timestrToHourUDF(F.col('arrival_time')))\n",
    "\n",
    "getDaysOfWeekUDF = F.udf(lambda x: get_days_of_week(x), ArrayType(IntegerType()))\n",
    "alltrips = alltrips.withColumn(\"dayofweek\", getDaysOfWeekUDF(F.array('monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday')))\n",
    "alltrips = alltrips.withColumn(\"dayofweek\", F.explode(\"dayofweek\"))\n",
    "\n",
    "alltrips.createOrReplaceTempView('alltrips_1')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT gtfs_date, dayofweek, hour, gtfs_route_id, gtfs_direction_id, count(trip_id) AS gtfs_number_of_scheduled_trips\n",
    "FROM alltrips_1\n",
    "GROUP BY gtfs_date, dayofweek, hour, gtfs_route_id, gtfs_direction_id\n",
    "\"\"\"\n",
    "trips_per_route = spark.sql(query)\n",
    "apcdata = apcdata.join(trips_per_route, on=['gtfs_date', 'dayofweek', 'hour', 'gtfs_route_id', 'gtfs_direction_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516764d0-34da-44c3-b133-e5a5b900c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scheduled trips per stop\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT gtfs_date, dayofweek, hour, gtfs_route_id, gtfs_direction_id, stop_id, count(trip_id) AS gtfs_number_of_scheduled_trips_at_stop\n",
    "FROM alltrips_1\n",
    "GROUP BY gtfs_date, dayofweek, hour, gtfs_route_id, gtfs_direction_id, stop_id\n",
    "\"\"\"\n",
    "trips_per_stop = spark.sql(query)\n",
    "apcdata = apcdata.join(trips_per_stop, on=['gtfs_date', 'dayofweek', 'hour', 'gtfs_route_id', 'gtfs_direction_id', 'stop_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd183f1",
   "metadata": {},
   "source": [
    "## Headway\n",
    "* Here we generate the actual and scheduled headways based on the arrival time and scheduled time in the APC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36297ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "apcdata = apcdata.withColumn(\"delay_time\", F.col(\"scheduled_time\").cast(\"long\") - F.col(\"arrival_time\").cast(\"long\")) # calculated in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaeb223",
   "metadata": {},
   "outputs": [],
   "source": [
    "apcdata = apcdata.withColumn(\"dwell_time\", F.col(\"departure_time\").cast(\"long\") - F.col(\"arrival_time\").cast(\"long\")) # calculated in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c1b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec_sched = Window.partitionBy( \"transit_date\", \"route_id\", \"route_direction_name\", \"stop_id\").orderBy(\"scheduled_time\")\n",
    "apcdata = apcdata.withColumn(\"prev_sched\", F.lag(\"scheduled_time\", 1).over(windowSpec_sched))\n",
    "apcdata = apcdata.withColumn(\"sched_hdwy\", F.col(\"scheduled_time\").cast(\"long\") - F.col(\"prev_sched\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c838262",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec_actual = Window.partitionBy( \"transit_date\", \"route_id\", \"route_direction_name\", \"stop_id\").orderBy(\"departure_time\")\n",
    "apcdata = apcdata.withColumn(\"prev_depart\", F.lag(\"departure_time\", 1).over(windowSpec_actual))\n",
    "apcdata = apcdata.withColumn(\"actual_hdwy\", F.col(\"departure_time\").cast(\"long\") - F.col(\"prev_depart\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120517d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "apcdata = apcdata.withColumn('is_gapped', F.when(((F.col('actual_hdwy') - F.col('sched_hdwy')) > 0) & ((F.col('actual_hdwy') / F.col('sched_hdwy')) >= 1.5), 1).otherwise(0))\n",
    "apcdata = apcdata.withColumn('is_bunched', F.when(((F.col('actual_hdwy') - F.col('sched_hdwy')) < 0) & ((F.col('actual_hdwy') / F.col('sched_hdwy')) >= 0) & ((F.col('actual_hdwy') / F.col('sched_hdwy')) <= 0.5), 1).otherwise(0))\n",
    "apcdata = apcdata.withColumn('is_target', F.when(((F.col('actual_hdwy') / F.col('sched_hdwy')) > 0.5) & ((F.col('actual_hdwy') / F.col('sched_hdwy')) < 1.5), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85128eed",
   "metadata": {},
   "source": [
    "## Cleaning but i think the dataset has been cleaned prior to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c598f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates=apcdata.groupby(['transit_date','trip_id','route_id','route_direction_name','stop_id_original', 'stop_sequence','block_abbr','vehicle_id']).count()\n",
    "todelete=duplicates.filter('count >1').select('transit_date','block_abbr').distinct()\n",
    "todelete=todelete.withColumn('indicator',F.lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duplicates)\n",
    "print(todelete.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for null vehicle id -- remove the whole block\n",
    "nullvehicleids=apcdata.filter('vehicle_id=\"NULL\" or vehicle_id is null').select('transit_date','block_abbr').distinct()\n",
    "nullvehicleids=nullvehicleids.withColumn('indicator',F.lit(1))\n",
    "nullvehicleids.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d782c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_arrival_departure_times=apcdata.groupBy('transit_date', 'trip_id','vehicle_id','overload_id','block_abbr')  .agg((F.sum(F.col('arrival_time').isNull().cast(\"int\")).alias('null_arrival_count')),F.count('*').alias('total_count'))\n",
    "null_arrival_departure_times=null_arrival_departure_times.filter('null_arrival_count = total_count').select('transit_date','block_abbr').distinct()\n",
    "null_arrival_departure_times=null_arrival_departure_times.withColumn('indicator',F.lit(1))\n",
    "null_arrival_departure_times.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72250b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "apcdata.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join(os.getcwd(), 'data', \"cleaned-merged-wego-daily.apc.parquet\")\n",
    "# apcdata.write.save(f)\n",
    "apcdata.write.partitionBy(\"year\", 'month').mode(\"overwrite\").parquet(fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
