{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generates the following files:\n",
    "* See commented out `to_pickle` lines.\n",
    "\n",
    "1. route_geoms_df.pkl\n",
    "2. lxlgrids_davison.pkl\n",
    "3. XDSegIds_for_all_trips.pkl\n",
    "\n",
    "### Requires `inrix_grouped.pkl`  \n",
    "    * This is an external file that lists all the road segment information covered by inrix.  \n",
    "\n",
    "### These 2 are just variations of the `triplevel_df.parquet`, which is generated in the `day_ahead` and `any_day` files.\n",
    "    1. triplevel_df_processed_time_window_with_IDs.pickle\n",
    "    2. triplevel_df_processed_MAIN_NOTEBOOK.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon, LineString\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '80g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\\\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generates 1x1 mile^2 grids across Nashville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'shapefiles', \"tncounty\")\n",
    "gdf_county = gpd.read_file(fp)\n",
    "gdf_county.plot()\n",
    "gdf_dav = gdf_county[gdf_county[\"NAME\"] == \"Davidson\"]\n",
    "gdf_david = gdf_dav.to_crs(\"EPSG:4326\")\n",
    "gdf_david.crs\n",
    "xmin, ymin, xmax, ymax = gdf_dav.total_bounds\n",
    "gdf_dav.total_bounds\n",
    "length = 5280\n",
    "wide = 5280\n",
    "\n",
    "cols = list(np.arange(xmin, xmax + wide, wide))\n",
    "rows = list(np.arange(ymin, ymax + length, length))\n",
    "\n",
    "polygons = []\n",
    "for x in cols[:-1]:\n",
    "    for y in rows[:-1]:\n",
    "        polygons.append(Polygon([(x,y), (x+wide, y), (x+wide, y+length), (x, y+length)]))\n",
    "\n",
    "grid = gpd.GeoDataFrame({'geometry':polygons})\n",
    "fp = os.path.join('data', 'shapefiles', 'grid_shapes.shp')\n",
    "grid.to_file(fp)\n",
    "grid.plot(ax = gdf_dav.plot(color='blue'), color='none', edgecolor='red')\n",
    "grids = grid.set_crs(\"EPSG:2274\")\n",
    "dav_grids = gpd.overlay(gdf_dav, grids, how='intersection')\n",
    "\n",
    "\n",
    "dav_grids['row_num'] = np.arange(len(dav_grids))\n",
    "dav_grids2 = dav_grids.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# fp = os.path.join('data', '1x1grids_davidson.pkl')\n",
    "# dav_grids2.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets all route geometries, probably easier from GTFS but it did not have all files i need (or i missed it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.path.join('..', '..', 'data', 'processed_parquet_JP_all')\n",
    "apcdata = spark.read.load(f)\n",
    "apcdata = apcdata.sort(\"arrival_time\")\\\n",
    "            .select(\"transit_date\", \"trip_id\", \"map_longitude\", \"map_latitude\")\\\n",
    "            .groupby('transit_date', 'trip_id')\\\n",
    "            .agg(F.collect_list(\"map_longitude\").alias(\"map_longitude\"), F.collect_list(\"map_latitude\").alias(\"map_latitude\"))\n",
    "\n",
    "apcdata = apcdata.drop(\"transit_date\")\n",
    "apcdata = apcdata.dropDuplicates(['trip_id'])\n",
    "def create_lineString(x):\n",
    "    geometry = [xy for xy in zip(x.map_longitude, x.map_latitude)]\n",
    "    return LineString(geometry)\n",
    "\n",
    "apcdf = apcdata.toPandas()\n",
    "apcdf['geometry'] = apcdf.apply(lambda x: create_lineString(x), axis=1)\n",
    "apcdf = apcdf.set_geometry('geometry')\n",
    "apcdf = apcdf.drop(columns=['map_longitude', 'map_latitude'], axis=1)\n",
    "# fp = os.path.join('data', 'route_geoms_df.pkl')\n",
    "# apcdf.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets all XDSegIDs (Inrix segments) used in the trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inrix segment data\n",
    "fp = os.path.join('data', 'inrix_grouped.pkl')\n",
    "with open(fp, \"rb\") as fh:\n",
    "  inrix_segment_df = pickle.load(fh)\n",
    "\n",
    "inrix_segment_df = inrix_segment_df.set_geometry('geometry')\n",
    "inrix_segment_df = inrix_segment_df[inrix_segment_df['County_inrix'] == 'davidson']\n",
    "davidson_segs = inrix_segment_df.XDSegID.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rtree would be faster\n",
    "def find_grids_intersecting(gdf, linestring):\n",
    "    spatial_index = gdf.sindex\n",
    "    possible_matches_index = list(spatial_index.intersection(linestring.bounds))\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    precise_matches = possible_matches[possible_matches.intersects(linestring)]\n",
    "    return precise_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate/load 1x1 mile grids\n",
    "fp = os.path.join('data', '1x1grids_davidson.pkl')\n",
    "grids_df = pd.read_pickle(fp)\n",
    "grids_df = grids_df.set_geometry('geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get route linestring data\n",
    "fp = os.path.join('data', 'route_geoms_df.pkl')\n",
    "trip_id_geom_data = pd.read_pickle(fp)\n",
    "trip_id_geom_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting segments in trips\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "# Get APC data\n",
    "fp = os.path.join('data', 'triplevel_df_processed_MAIN_NOTEBOOK.pickle')\n",
    "df = pd.read_pickle(fp)\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates(subset=['trip_id', 'route_id_direction'], keep='first')\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "CORES = cpu_count()\n",
    "\n",
    "def merge_cluster(idx):\n",
    "    trip_ids = df.iloc[idx].trip_id.tolist()\n",
    "    all_used_segments = []\n",
    "    for trip_id in trip_ids:\n",
    "        route_linestring = trip_id_geom_data[trip_id_geom_data['trip_id'] == trip_id]['geometry'].values[0]\n",
    "        if route_linestring is None: \n",
    "            print(\"trip id LS not found.\")\n",
    "        \n",
    "        route_grids = find_grids_intersecting(grids_df, route_linestring)\n",
    "        if route_grids.empty: \n",
    "            print(\"route grids for trip not found.\")\n",
    "        \n",
    "        route_segments = inrix_segment_df[inrix_segment_df['geometry'].within(route_grids.unary_union)]['XDSegID'].tolist()\n",
    "        if len(route_segments) == 0: \n",
    "            print(\"route segments for trip not found.\")\n",
    "        \n",
    "        all_used_segments = list(set(all_used_segments + route_segments))\n",
    "    return all_used_segments\n",
    "o_index_group = np.array_split(df.index, CORES)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=CORES) as pool:\n",
    "    results = pool.map(merge_cluster, o_index_group)\n",
    "results = list(results)\n",
    "out = []\n",
    "[out.extend(r) for r in results]\n",
    "results = list(set(out))\n",
    "fp = os.path.join('data', 'XDSegIDs_for_all_trips.pkl')\n",
    "with open(fp, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
