{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0938411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/seconddrive/mta_stationing_problem\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e19e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime as dt\n",
    "import importlib\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "import IPython\n",
    "from copy import deepcopy\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from src import tf_utils, config, data_utils, models, linklevel_utils\n",
    "\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "pd.set_option('display.max_columns', None)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c37ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.models' from '/media/seconddrive/mta_stationing_problem/src/models.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(tf_utils)\n",
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa82f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bedcd47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/09 21:00:28 WARN Utils: Your hostname, scope-vanderbilt resolves to a loopback address: 127.0.1.1; using 10.2.218.69 instead (on interface enp8s0)\n",
      "22/09/09 21:00:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/09 21:00:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '40g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.path.join('data', 'processed', 'apc_weather_gtfs.parquet')\n",
    "apcdata = spark.read.load(f)\n",
    "todelete = apcdata.filter('(load < 0) OR (load IS NULL)').select('transit_date','trip_id','overload_id').distinct()\n",
    "todelete=todelete.withColumn('marker',F.lit(1))\n",
    "\n",
    "#joining and whereever the records are not found in sync error table the marker will be null\n",
    "apcdataafternegdelete=apcdata.join(todelete,on=['trip_id','transit_date','overload_id'],how='left').filter('marker is null').drop('marker')\n",
    "apcdataafternegdelete = apcdataafternegdelete.sort(['trip_id', 'overload_id'])\n",
    "apcdataafternegdelete.show(1, vertical=True, truncate=False)\n",
    "get_columns = ['trip_id', 'transit_date', 'arrival_time', \n",
    "               'block_abbr', 'stop_sequence', 'stop_id_original',\n",
    "               'load', \n",
    "               'darksky_temperature', \n",
    "               'darksky_humidity', \n",
    "               'darksky_precipitation_probability', \n",
    "               'route_direction_name', 'route_id',\n",
    "               'dayofweek',  'year', 'month', 'hour', 'zero_load_at_trip_end',\n",
    "               'sched_hdwy']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "\n",
    "apcdataafternegdelete.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "# # filter subset\n",
    "query = f\"\"\"\n",
    "SELECT {get_str}\n",
    "FROM apc\n",
    "\"\"\"\n",
    "print(query)\n",
    "\n",
    "apcdataafternegdelete = spark.sql(query)\n",
    "apcdataafternegdelete = apcdataafternegdelete.na.fill(value=0,subset=[\"zero_load_at_trip_end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbbd659",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apcdataafternegdelete.toPandas()\n",
    "print(df.shape)\n",
    "old_shape = df.shape[0]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.arrival_time.notna()]\n",
    "df = df[df.sched_hdwy.notna()]\n",
    "df = df[df.darksky_temperature.notna()]\n",
    "\n",
    "df['route_id_dir'] = df[\"route_id\"].astype(\"str\") + \"_\" + df[\"route_direction_name\"]\n",
    "df['day'] = df[\"arrival_time\"].dt.day\n",
    "df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "# Adding extra features\n",
    "# Holidays\n",
    "fp = os.path.join('data', 'others', 'US Holiday Dates (2004-2021).csv')\n",
    "holidays_df = pd.read_csv(fp)\n",
    "holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])\n",
    "holidays_df['is_holiday'] = True\n",
    "df = df.merge(holidays_df[['Date', 'is_holiday']], left_on='transit_date', right_on='Date', how='left')\n",
    "df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "df = df.drop(columns=['Date'])\n",
    "    \n",
    "# School breaks\n",
    "fp = os.path.join('data', 'others', 'School Breaks (2019-2022).pkl')\n",
    "school_break_df = pd.read_pickle(fp)\n",
    "school_break_df['is_school_break'] = True\n",
    "df = df.merge(school_break_df[['Date', 'is_school_break']], left_on='transit_date', right_on='Date', how='left')\n",
    "df['is_school_break'] = df['is_school_break'].fillna(False)\n",
    "df = df.drop(columns=['Date'])\n",
    "\n",
    "# Traffic\n",
    "# Causes 3M data points to be lost\n",
    "fp = os.path.join('data', 'traffic', 'triplevel_speed.pickle')\n",
    "speed_df = pd.read_pickle(fp)\n",
    "speed_df = speed_df.rename({'route_id_direction':'route_id_dir'}, axis=1)\n",
    "speed_df = speed_df[['transit_date', 'trip_id', 'route_id_dir', 'traffic_speed']]\n",
    "df = df.merge(speed_df, how='left', \n",
    "                left_on=['transit_date', 'trip_id', 'route_id_dir'], \n",
    "                right_on=['transit_date', 'trip_id', 'route_id_dir'])\n",
    "# df = df[~df['traffic_speed'].isna()]\n",
    "df['traffic_speed'].bfill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_shape - df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = []\n",
    "for ba in tqdm(df.block_abbr.unique()):\n",
    "    ba_df = df[df['block_abbr'] == ba]\n",
    "    end_stop = ba_df.stop_sequence.max()\n",
    "    # Same result as creating a fixed_arrival_time (but faster)\n",
    "    ba_df = ba_df[ba_df.stop_sequence != end_stop].reset_index(drop=True)\n",
    "    sorted_df.append(ba_df)\n",
    "        \n",
    "overall_df = pd.concat(sorted_df)\n",
    "drop_cols = ['route_direction_name', 'route_id', 'trip_id']\n",
    "drop_cols = [col for col in drop_cols if col in overall_df.columns]\n",
    "overall_df = overall_df.drop(drop_cols, axis=1)\n",
    "\n",
    "# overall_df = overall_df.rename({\"fixed_arrival_time\": \"arrival_time\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEWINDOW = 15\n",
    "overall_df['minute'] = overall_df['arrival_time'].dt.minute\n",
    "overall_df['minuteByWindow'] = overall_df['minute'] // TIMEWINDOW\n",
    "overall_df['temp'] = overall_df['minuteByWindow'] + (overall_df['hour'] * 60 / TIMEWINDOW)\n",
    "overall_df['time_window'] = np.floor(overall_df['temp']).astype('int')\n",
    "overall_df = overall_df.drop(columns=['minute', 'minuteByWindow', 'temp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate stops by time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(overall_df.shape)\n",
    "overall_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by time windows and get the maximum of the aggregate load/class/sched\n",
    "# Get mean of temperature (mostly going to be equal)\n",
    "# TODO: Double check this! \n",
    "overall_df = overall_df.groupby(['transit_date', \n",
    "                                 'route_id_dir', \n",
    "                                 'stop_id_original',\n",
    "                                 'time_window']).agg({\"block_abbr\":\"first\",\n",
    "                                                      \"arrival_time\":\"first\",\n",
    "                                                      \"year\":\"first\", \n",
    "                                                      \"month\":\"first\",\n",
    "                                                      \"day\": \"first\",\n",
    "                                                      \"hour\":\"first\",\n",
    "                                                      \"is_holiday\": \"first\",\n",
    "                                                      \"is_school_break\":\"first\",\n",
    "                                                      \"dayofweek\":\"first\",\n",
    "                                                      \"zero_load_at_trip_end\":\"first\",\n",
    "                                                      \"stop_sequence\":\"first\",\n",
    "                                                      \"darksky_temperature\":\"mean\", \n",
    "                                                      \"darksky_humidity\":\"mean\",\n",
    "                                                      \"darksky_precipitation_probability\": \"mean\",\n",
    "                                                      \"traffic_speed\":\"mean\",\n",
    "                                                      \"sched_hdwy\": \"max\",\n",
    "                                                      \"load\": \"sum\" })\n",
    "overall_df = overall_df.reset_index(level=[0,1,2,3])\n",
    "overall_df = overall_df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(overall_df.shape)\n",
    "overall_df[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['arrival_time', 'block_abbr']\n",
    "drop_cols = [col for col in drop_cols if col in overall_df.columns]\n",
    "overall_df = overall_df.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4130605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking bins of loads for possible classification problem\n",
    "loads = overall_df[overall_df.load <= config.TARGET_MAX]['load']\n",
    "percentiles = []\n",
    "for cbin in config.CLASS_BINS:\n",
    "    percentile = np.percentile(loads.values, cbin)\n",
    "    percentiles.append(percentile)\n",
    "\n",
    "# percentiles = [(percentiles[0], percentiles[1]), (percentiles[1] + 1, percentiles[2]), (percentiles[2] + 1, percentiles[3])]\n",
    "percentiles = [(percentiles[0], percentiles[1]), (percentiles[1] + 1, percentiles[2]), (percentiles[2] + 1, 55.0), (56.0, 75.0), (76.0, 100.0)]\n",
    "print(f\"Percentiles: {percentiles}\")\n",
    "overall_df[config.TARGET_COLUMN_CLASSIFICATION] = overall_df['load'].apply(lambda x: data_utils.get_class(x, percentiles))\n",
    "overall_df = overall_df[overall_df[config.TARGET_COLUMN_CLASSIFICATION].notna()]\n",
    "overall_df.y_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df.y_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "past = 10 # Past stops observed\n",
    "future = 1 # Future stops predicted\n",
    "offset = 0\n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "\n",
    "feature_label = config.TARGET_COLUMN_CLASSIFICATION\n",
    "patience = 10\n",
    "\n",
    "hyperparams_dict = {'past': past,\n",
    "                    'future': future,\n",
    "                    'offset': offset,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'epochs': epochs,\n",
    "                    'patience': patience}\n",
    "hyperparams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = config.TARGET_COLUMN_CLASSIFICATION\n",
    "target = 'y_class'\n",
    "\n",
    "num_columns = ['darksky_temperature', 'darksky_humidity', 'darksky_precipitation_probability', 'sched_hdwy', 'traffic_speed']\n",
    "cat_columns = ['month', 'hour', 'day', 'stop_sequence', 'stop_id_original', 'year', target]\n",
    "ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end', 'time_window']\n",
    "\n",
    "columns = num_columns + cat_columns + ohe_columns\n",
    "print(f\"Numerical columns: {num_columns}\")\n",
    "print(f\"Categorical columns: {cat_columns}\")\n",
    "print(f\"One Hot Encode columns: {ohe_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df.hour.unique(), overall_df.stop_sequence.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dates = ('2020-01-01', '2021-06-30')\n",
    "val_dates =   ('2021-06-30', '2021-10-31')\n",
    "test_dates =  ('2021-10-31', '2022-04-06')\n",
    "\n",
    "ohe_encoder, label_encoder, num_scaler, train_df, val_df, test_df = linklevel_utils.prepare_linklevel(overall_df, \n",
    "                                                                                                 train_dates=train_dates, \n",
    "                                                                                                 val_dates=val_dates, \n",
    "                                                                                                 test_dates=test_dates,\n",
    "                                                                                                 cat_columns=cat_columns,\n",
    "                                                                                                 num_columns=num_columns,\n",
    "                                                                                                 ohe_columns=ohe_columns,\n",
    "                                                                                                 feature_label='y_class',\n",
    "                                                                                                 time_feature_used='transit_date',\n",
    "                                                                                                 scaler='minmax')\n",
    "\n",
    "drop_cols = ['transit_date', 'load', 'arrival_time']\n",
    "drop_cols = [col for col in drop_cols if col in train_df.columns]\n",
    "train_df = train_df.drop(drop_cols, axis=1)\n",
    "val_df = val_df.drop(drop_cols, axis=1)\n",
    "test_df = test_df.drop(drop_cols, axis=1)\n",
    "\n",
    "arrange_cols = [target] + [col for col in train_df.columns if col != target]\n",
    "train_df = train_df[arrange_cols]\n",
    "val_df = val_df[arrange_cols]\n",
    "test_df = test_df[arrange_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['y_class'] = train_df.y_class.astype('int')\n",
    "val_df['y_class']   = val_df.y_class.astype('int')\n",
    "test_df['y_class']  = test_df.y_class.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving encoders, scalers and column arrangement\n",
    "fp = os.path.join('models', 'same_day', 'LL_OHE_encoder.joblib')\n",
    "joblib.dump(ohe_encoder, fp)\n",
    "fp = os.path.join('models', 'same_day', 'LL_Label_encoders.joblib')\n",
    "joblib.dump(label_encoder, fp)\n",
    "fp = os.path.join('models', 'same_day', 'LL_Num_scaler.joblib')\n",
    "joblib.dump(num_scaler, fp)\n",
    "fp = os.path.join('models', 'same_day', 'LL_X_columns.joblib')\n",
    "joblib.dump(train_df.columns, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can add shuffle in the future\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def timeseries_dataset_from_dataset(df, feature_slice, label_slice, input_sequence_length, output_sequence_length, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "    ds = dataset.window(input_sequence_length + output_sequence_length, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x).batch(input_sequence_length + output_sequence_length)\n",
    "     \n",
    "    def split_feature_label(x):\n",
    "        return x[:input_sequence_length:, feature_slice], x[input_sequence_length:,label_slice]\n",
    "     \n",
    "    ds = ds.map(split_feature_label)\n",
    "     \n",
    "    return ds.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = train_df.columns.tolist().index(target)\n",
    "print(label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_slice = slice(label_index, label_index + 1, None) # which column the label/labels are\n",
    "feature_slice = slice(None, None, None) # Which feature columns are included, by default includes all (even label)\n",
    "input_sequence_length = past # number of past information to look at\n",
    "output_sequence_length = future # number of time steps to predict\n",
    "\n",
    "dataset_train = timeseries_dataset_from_dataset(train_df, \n",
    "                                                feature_slice=feature_slice,\n",
    "                                                label_slice=label_slice,\n",
    "                                                input_sequence_length=input_sequence_length, \n",
    "                                                output_sequence_length=output_sequence_length, \n",
    "                                                batch_size=batch_size)\n",
    "\n",
    "dataset_val = timeseries_dataset_from_dataset(val_df, \n",
    "                                              feature_slice=feature_slice,\n",
    "                                              label_slice=label_slice,\n",
    "                                              input_sequence_length=input_sequence_length, \n",
    "                                              output_sequence_length=output_sequence_length, \n",
    "                                              batch_size=batch_size)\n",
    "\n",
    "dataset_test = timeseries_dataset_from_dataset(test_df,\n",
    "                                               feature_slice=feature_slice,\n",
    "                                               label_slice=label_slice,\n",
    "                                               input_sequence_length=input_sequence_length, \n",
    "                                               output_sequence_length=output_sequence_length, \n",
    "                                               batch_size=batch_size)\n",
    "for batch in dataset_train.take(1):\n",
    "    (x, y) = batch\n",
    "    display(pd.DataFrame(x[100], columns=train_df.columns))\n",
    "    print(x[100].shape)\n",
    "    display(pd.DataFrame(y[100], columns=['y_class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_df.y_class.unique())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "# model.compile(\n",
    "#     loss=\"mean_absolute_error\",\n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "#     metrics=tf.keras.metrics.MeanSquaredError(),\n",
    "# )\n",
    "\n",
    "input_shape = (None, None, len(train_df.columns))\n",
    "model.build(input_shape)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_filepath = 'models/same_day/model/CLA_cp-epoch{epoch:02d}-loss{val_loss:.2f}.ckpt'\n",
    "checkpoint_filepath = 'models/same_day/school_zero_load/CLA_cp-epoch{epoch:02d}-loss{val_loss:.2f}.ckpt'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True), model_checkpoint_callback]\n",
    "\n",
    "history = model.fit(dataset_train, validation_data=dataset_val, epochs=epochs, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "* Load model and encoders,scalers,converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TIMEWINDOW = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "latest = tf.train.latest_checkpoint('models/same_day/model')\n",
    "columns = joblib.load('models/same_day/LL_X_columns.joblib')\n",
    "label_encoders = joblib.load('models/same_day/LL_Label_encoders.joblib')\n",
    "ohe_encoder = joblib.load('models/same_day/LL_OHE_encoder.joblib')\n",
    "num_scaler = joblib.load('models/same_day/LL_Num_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apc_data_for_date(filter_date):\n",
    "    print(\"Running this...\")\n",
    "    filepath = os.path.join('data', 'processed', 'apc_weather_gtfs.parquet')\n",
    "    apcdata = spark.read.load(filepath)\n",
    "    apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "    plot_date = filter_date.strftime('%Y-%m-%d')\n",
    "    get_columns = ['trip_id', 'transit_date', 'arrival_time', 'vehicle_id', 'ons',\n",
    "                   'block_abbr', 'stop_sequence', 'stop_name', 'stop_id_original',\n",
    "                   'load', \n",
    "                   'darksky_temperature', \n",
    "                   'darksky_humidity', \n",
    "                   'darksky_precipitation_probability', \n",
    "                   'route_direction_name', 'route_id', 'gtfs_direction_id',\n",
    "                   'dayofweek',  'year', 'month', 'hour',\n",
    "                   'sched_hdwy']\n",
    "    get_str = \", \".join([c for c in get_columns])\n",
    "    query = f\"\"\"\n",
    "    SELECT {get_str}\n",
    "    FROM apc\n",
    "    WHERE (transit_date == '{plot_date}')\n",
    "    ORDER BY arrival_time\n",
    "    \"\"\"\n",
    "    apcdata = spark.sql(query)\n",
    "    apcdata = apcdata.withColumn(\"route_id_dir\", F.concat_ws(\"_\", apcdata.route_id, apcdata.route_direction_name))\n",
    "    apcdata = apcdata.withColumn(\"day\", F.dayofmonth(apcdata.arrival_time))\n",
    "    apcdata = apcdata.drop(\"route_direction_name\")\n",
    "    apcdata = apcdata.withColumn(\"load\", F.when(apcdata.load < 0, 0).otherwise(apcdata.load))\n",
    "    return apcdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(input_df, ohe_encoder, label_encoders, num_scaler, columns, keep_columns=[], target='y_class'):\n",
    "    num_columns = ['darksky_temperature', 'darksky_humidity', 'darksky_precipitation_probability', 'sched_hdwy', 'traffic_speed']\n",
    "    cat_columns = ['month', 'hour', 'day', 'stop_sequence', 'stop_id_original', 'year', 'time_window']\n",
    "    ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday']\n",
    "\n",
    "    # OHE\n",
    "    input_df[ohe_encoder.get_feature_names_out()] = ohe_encoder.transform(input_df[ohe_columns]).toarray()\n",
    "    # input_df = input_df.drop(columns=ohe_columns)\n",
    "\n",
    "    # Label encoder\n",
    "    for cat in cat_columns:\n",
    "        encoder = label_encoders[cat]\n",
    "        input_df[cat] = encoder.transform(input_df[cat])\n",
    "    \n",
    "    # Num scaler\n",
    "    input_df[num_columns] = num_scaler.transform(input_df[num_columns])\n",
    "    input_df['y_class']  = input_df.y_class.astype('int')\n",
    "\n",
    "    if keep_columns:\n",
    "        columns = keep_columns + columns\n",
    "    # Rearrange columns\n",
    "    input_df = input_df[columns]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def assign_data_to_bins(df, TARGET='load'):\n",
    "    bins = pd.IntervalIndex.from_tuples([(-1, 6.0), (7.0, 12.0), (13.0, 55.0), (56.0, 75.0), (76.0, 100.0)])\n",
    "    mycut = pd.cut(df[TARGET].tolist(), bins=bins)\n",
    "    df['y_class'] = mycut.codes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_predict = dt.date(2021, 10, 18)\n",
    "apcdata = get_apc_data_for_date(date_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apcdata.toPandas()\n",
    "df = df[df.arrival_time.notna()]\n",
    "df = df[df.sched_hdwy.notna()]\n",
    "df = df[df.darksky_temperature.notna()]\n",
    "\n",
    "df['day'] = df[\"arrival_time\"].dt.day\n",
    "df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "# Adding extra features\n",
    "# Holidays\n",
    "fp = os.path.join('data', 'others', 'US Holiday Dates (2004-2021).csv')\n",
    "holidays_df = pd.read_csv(fp)\n",
    "holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])\n",
    "holidays_df['is_holiday'] = True\n",
    "df = df.merge(holidays_df[['Date', 'is_holiday']], left_on='transit_date', right_on='Date', how='left')\n",
    "df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "df = df.drop(columns=['Date'])\n",
    "\n",
    "# Traffic\n",
    "# Causes 3M data points to be lost\n",
    "fp = os.path.join('data', 'traffic', 'triplevel_speed.pickle')\n",
    "speed_df = pd.read_pickle(fp)\n",
    "speed_df = speed_df.rename({'route_id_direction':'route_id_dir'}, axis=1)\n",
    "speed_df = speed_df[['transit_date', 'trip_id', 'route_id_dir', 'traffic_speed']]\n",
    "df = df.merge(speed_df, how='left', \n",
    "                left_on=['transit_date', 'trip_id', 'route_id_dir'], \n",
    "                right_on=['transit_date', 'trip_id', 'route_id_dir'])\n",
    "# df = df[~df['traffic_speed'].isna()]\n",
    "df['traffic_speed'].bfill(inplace=True)\n",
    "\n",
    "df['minute'] = df['arrival_time'].dt.minute\n",
    "df['minuteByWindow'] = df['minute'] // TIMEWINDOW\n",
    "df['temp'] = df['minuteByWindow'] + (df['hour'] * 60 / TIMEWINDOW)\n",
    "df['time_window'] = np.floor(df['temp']).astype('int')\n",
    "df = df.drop(columns=['minute', 'minuteByWindow', 'temp'])\n",
    "\n",
    "# HACK\n",
    "df = df[df['hour'] != 3]\n",
    "df = df[df['stop_sequence'] != 0]\n",
    "\n",
    "df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "df = assign_data_to_bins(df, TARGET='load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = prepare_input_data(df, ohe_encoder, label_encoders, num_scaler, columns, target='y_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_lstm_predictions(input_df, model, past, future):\n",
    "    past_df = input_df[0:past]\n",
    "    future_df = input_df[past:]\n",
    "    predictions = []\n",
    "    for f in range(future):\n",
    "        pred = model.predict(past_df.to_numpy().reshape(1, *past_df.shape))\n",
    "        y_pred = np.argmax(pred)\n",
    "        predictions.append(y_pred)\n",
    "        \n",
    "        # Add information from future\n",
    "        last_row = future_df.iloc[[0]]\n",
    "        last_row['y_class'] = y_pred\n",
    "        past_df = pd.concat([past_df[1:], last_row])\n",
    "        \n",
    "        # Move future to remove used row\n",
    "        future_df = future_df[1:]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = input_df[0:20]\n",
    "model = linklevel_utils.setup_simple_lstm_generator(input_df.shape[1], 5)\n",
    "model.load_weights(latest)\n",
    "y_pred = generate_simple_lstm_predictions(tdf, model, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up grid search config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'past': 1,\n",
       " 'time_window': 10,\n",
       " 'layer': 128,\n",
       " 'batch_size': 256,\n",
       " 'learning_rate': 0.01,\n",
       " 'epochs': 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "past = [1, 3, 5, 7, 9, 11]\n",
    "time_window = [10, 20, 30, 40, 50, 60]\n",
    "# layer = [64, 128, 256]\n",
    "# batch_size = [256, 512]\n",
    "# learning_rate = [0.1, 0.01, 0.001, 0.0001]\n",
    "layer = [128]\n",
    "batch_size = [256]\n",
    "learning_rate = [0.01]\n",
    "epochs = [10]\n",
    "config = [dict(zip(('past',\n",
    "           'time_window', \n",
    "           'layer',\n",
    "           'batch_size',\n",
    "           'learning_rate',\n",
    "           'epochs'), (_past, \n",
    "                      _time_window, \n",
    "                      _layer,\n",
    "                      _batch_size,\n",
    "                      _learning_rate,\n",
    "                      _epochs))) for _past, _time_window, _layer, _batch_size, _learning_rate, _epochs in product(past, \n",
    "                                                                                                                  time_window, \n",
    "                                                                                                                  layer,\n",
    "                                                                                                                  batch_size,\n",
    "                                                                                                                  learning_rate,\n",
    "                                                                                                                  epochs)]\n",
    "len(config)\n",
    "config[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
