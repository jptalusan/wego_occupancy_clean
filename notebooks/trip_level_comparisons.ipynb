{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from src import data_utils, triplevel_utils\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import datetime as dt\n",
    "import swifter\n",
    "import xgboost as xgb\n",
    "from keras import backend as K \n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, concatenate, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from pandas import MultiIndex, Int64Index\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 15:52:11 WARN Utils: Your hostname, scope-vanderbilt resolves to a loopback address: 127.0.1.1; using 10.2.218.69 instead (on interface enp8s0)\n",
      "22/09/15 15:52:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 15:52:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/09/15 15:52:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '40g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '20g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Utils\"\n",
    "def get_time_window(row, window):\n",
    "    minute = row.arrival_time.minute\n",
    "    minuteByWindow = minute//window\n",
    "    temp = minuteByWindow + (row.hour * (60/window))\n",
    "    return round(temp, 2)\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/15 15:52:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load the APC data from a prepared file\n",
    "processed_triplevel = os.path.join('../data', 'processed', 'triplevel_df.parquet')\n",
    "if not os.path.exists(processed_triplevel):\n",
    "# if True:\n",
    "    filepath = os.path.join(os.getcwd(), \"../data\", \"processed\", \"apc_weather_gtfs.parquet\")\n",
    "    apcdata = spark.read.load(filepath)\n",
    "    apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "    # filter subset\n",
    "    query = f\"\"\"\n",
    "                SELECT *\n",
    "                FROM apc\n",
    "            \"\"\"\n",
    "    apcdata=spark.sql(query)\n",
    "    apcdata = data_utils.remove_nulls_from_apc(apcdata)\n",
    "    apcdata.createOrReplaceTempView('apcdata')\n",
    "    apcdata_per_trip = data_utils.get_apc_per_trip_sparkview(spark)\n",
    "    df = apcdata_per_trip.toPandas()\n",
    "    \n",
    "    # Adding extra features\n",
    "    # Holidays\n",
    "    fp = os.path.join('../data', 'others', 'US Holiday Dates (2004-2021).csv')\n",
    "    holidays_df = pd.read_csv(fp)\n",
    "    holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])\n",
    "    holidays_df['is_holiday'] = True\n",
    "    df = df.merge(holidays_df[['Date', 'is_holiday']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "    \n",
    "    # School breaks\n",
    "    fp = os.path.join('../data', 'others', 'School Breaks (2019-2022).pkl')\n",
    "    school_break_df = pd.read_pickle(fp)\n",
    "    school_break_df['is_school_break'] = True\n",
    "    df = df.merge(school_break_df[['Date', 'is_school_break']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_school_break'] = df['is_school_break'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "\n",
    "    # Traffic\n",
    "    fp = os.path.join('../data', 'traffic', 'triplevel_speed.pickle')\n",
    "    speed_df = pd.read_pickle(fp)\n",
    "    speed_df = speed_df[['transit_date', 'trip_id', 'route_id_direction', 'traffic_speed']]\n",
    "    df = df.merge(speed_df, how='left', \n",
    "                  left_on =['transit_date', 'trip_id', 'route_id_direction'], \n",
    "                  right_on=['transit_date', 'trip_id', 'route_id_direction'])\n",
    "    df = df[~df['traffic_speed'].isna()]\n",
    "    df.to_parquet(processed_triplevel, engine='auto', compression='gzip')\n",
    "else:\n",
    "    df = pd.read_parquet(processed_triplevel, engine='auto')\n",
    "    df = df.dropna()\n",
    "    # Removing time_window in case a different one will be used\n",
    "df = df.drop(['time_window', 'load'], axis=1)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['route_id_direction', 'is_holiday', 'dayofweek', 'is_school_break', 'time_window']\n",
    "ord_features = ['year', 'month', 'hour', 'day']\n",
    "# num_features = ['temperature', 'humidity', 'precipitation_intensity', 'scheduled_headway', 'traffic_speed']\n",
    "num_features = ['temperature', 'humidity', 'precipitation_intensity', 'scheduled_headway']\n",
    "\n",
    "RANDOM_SEED = 100\n",
    "WINDOW = 30\n",
    "PAST_TRIPS = 5\n",
    "TARGET = 'y_reg100'\n",
    "FOLDS = 3\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = triplevel_utils.generate_new_features(tdf, time_window=WINDOW, past_trips=PAST_TRIPS, target=TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by time windows and get the maximum of the aggregate load/class/sched\n",
    "# Get mean of temperature (mostly going to be equal)\n",
    "tdf = tdf.groupby(['transit_date', 'route_id_direction', 'time_window']).agg({\"trip_id\":\"first\",\n",
    "                                                                              \"year\":\"first\", \n",
    "                                                                              \"month\":\"first\",\n",
    "                                                                              \"day\": \"first\",\n",
    "                                                                              \"dayofweek\":\"first\", \n",
    "                                                                              \"hour\":\"first\",\n",
    "                                                                              \"is_holiday\": \"first\",\n",
    "                                                                              \"is_school_break\": \"first\",\n",
    "                                                                              \"temperature\":\"mean\", \n",
    "                                                                              \"humidity\":\"mean\",\n",
    "                                                                              \"precipitation_intensity\": \"mean\",\n",
    "                                                                              \"scheduled_headway\": \"max\",\n",
    "                                                                              TARGET: \"max\"})\n",
    "                                                                            #   \"traffic_speed\":\"mean\",\n",
    "tdf = tdf.reset_index(level=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ohe_encoder is for the following column order: ['route_id_direction', 'is_holiday', 'dayofweek', 'is_school_break', 'time_window']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.0, 9.0), (10.0, 16.0), (16.0, 55.0), (56.0, 75.0), (76.0, 100.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ohe_encoder is for the following column order:\", cat_features)\n",
    "\n",
    "rf_df, ix_map, ohe_encoder, percentiles = triplevel_utils.prepare_df_for_training(tdf, cat_features, ord_features, target=TARGET)\n",
    "rf_df, percentiles = triplevel_utils.adjust_bins(rf_df, TARGET=TARGET, percentiles=percentiles)\n",
    "\n",
    "original_rf = deepcopy(rf_df)\n",
    "original_rf['time_window'] = tdf['time_window']\n",
    "\n",
    "drop_cols = ['time_window', 'route_id', 'route_direction_name', 'block_abbr', 'y_reg100', 'y_reg095', 'transit_date', 'is_holiday', 'route_id_direction', 'actual_headways', 'trip_id', 'arrival_time']\n",
    "drop_cols = [col for col in drop_cols if col in rf_df.columns]\n",
    "rf_df = rf_df.drop(drop_cols, axis=1)\n",
    "\n",
    "percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop informational columns\n",
    "y = rf_df.pop('y_class')\n",
    "X = rf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_original_data_from_RF(df, ix_map, ohe_encoder):\n",
    "    df[cat_features] = ohe_encoder.inverse_transform(df.filter(regex='route_id_direction_|is_holiday_|dayofweek_|is_school_break_|time_window_'))\n",
    "    \n",
    "    for col in ord_features:\n",
    "        inv_map = {v: k for k, v in ix_map[col].items()}\n",
    "        df[col] = df[f\"{col}_ix\"].apply(lambda x: inv_map[x])\n",
    "        \n",
    "    df = df.drop(columns=df.filter(regex='route_id_direction_|is_holiday_|dayofweek_|is_school_break_|time_window_|_ix').columns, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=FOLDS, test_size=0.3, random_state=RANDOM_SEED)\n",
    "sss.get_n_splits(X, y)\n",
    "\n",
    "kfold = 0\n",
    "results_df_arr = []\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=8)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    res_df = original_rf.iloc[test_index]\n",
    "    res_df['y_pred'] = y_pred\n",
    "    res_df['y_true'] = y_test\n",
    "    res_df['kfold'] = kfold\n",
    "    res_df = reconstruct_original_data_from_RF(res_df, ix_map, ohe_encoder)\n",
    "    kfold = kfold + 1\n",
    "    results_df_arr.append(res_df)\n",
    "    \n",
    "fp = os.path.join('../evaluation', 'any_day_comparisons', 'RF_raw_results.pkl')\n",
    "pd.concat(results_df_arr).to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('../evaluation', 'any_day_comparisons', 'RF_raw_results.pkl')\n",
    "rf_results = pd.read_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=FOLDS, test_size=0.3, random_state=RANDOM_SEED)\n",
    "sss.get_n_splits(X, y)\n",
    "\n",
    "objective = 'multi:softmax'\n",
    "kfold = 0\n",
    "results_df_arr = []\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = xgb.XGBClassifier(n_estimators=600,\n",
    "                              max_depth=12,\n",
    "                              use_label_encoder=False, \n",
    "                              objective=objective, \n",
    "                              eval_metric='mlogloss', \n",
    "                              num_class=len(y.unique()))\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    res_df = original_rf.iloc[test_index]\n",
    "    res_df['y_pred'] = y_pred\n",
    "    res_df['y_true'] = y_test\n",
    "    res_df['kfold'] = kfold\n",
    "    res_df = reconstruct_original_data_from_RF(res_df, ix_map, ohe_encoder)\n",
    "    kfold = kfold + 1\n",
    "    results_df_arr.append(res_df)\n",
    "fp = os.path.join('../evaluation', 'any_day_comparisons', 'XGB_raw_results.pkl')\n",
    "pd.concat(results_df_arr).to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('../evaluation', 'any_day_comparisons', 'XGB_raw_results.pkl')\n",
    "xgb_results = pd.read_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "def create_MLP(X, y):\n",
    "    # Create embeddings for categorical features\n",
    "    number_of_unique_rids = X.filter(like='route_id_direction_').shape[1]\n",
    "    number_of_unique_holidays = X.filter(like='is_holiday_').shape[1]\n",
    "    number_of_unique_dow = X.filter(like='dayofweek_').shape[1]\n",
    "    number_of_unique_break = X.filter(like='is_school_break_').shape[1]\n",
    "    number_of_unique_time_window = X.filter(like='time_window_').shape[1]\n",
    "    classes = len(y.unique())\n",
    "    print(f\"No. of classes: {classes}\")\n",
    "\n",
    "    numerical_input = Input(len(num_features) + len(ord_features), name='numerical_input')\n",
    "    \n",
    "    onehot_input1 = Input((number_of_unique_rids), name='ohe_rid')\n",
    "    ohe_layer1 = Dense(64)(onehot_input1)\n",
    "    \n",
    "    onehot_input2 = Input((number_of_unique_holidays), name='ohe_holiday')\n",
    "    ohe_layer2 = Dense(64)(onehot_input2)\n",
    "    \n",
    "    onehot_input3 = Input((number_of_unique_dow), name='ohe_dow')\n",
    "    ohe_layer3 = Dense(64)(onehot_input3)\n",
    "    \n",
    "    onehot_input4 = Input((number_of_unique_break), name='ohe_break')\n",
    "    ohe_layer4 = Dense(64)(onehot_input4)\n",
    "    \n",
    "    onehot_input5 = Input((number_of_unique_time_window), name='ohe_tw')\n",
    "    ohe_layer5 = Dense(64)(onehot_input5)\n",
    "\n",
    "    merged_input = concatenate([numerical_input, ohe_layer1, ohe_layer2, ohe_layer3, ohe_layer4, ohe_layer5], axis=1)\n",
    "    x = Dense(64, activation='relu')(merged_input)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(32, activation='relu')(merged_input)\n",
    "    output = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[numerical_input, onehot_input1, onehot_input2, onehot_input3, onehot_input4, onehot_input5], outputs=output, name='Simple_NN')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of classes: 5\n",
      "Epoch 1/10\n",
      "1605/1605 [==============================] - 5s 3ms/step - loss: 1.0142 - sparse_categorical_accuracy: 0.5048 - val_loss: 0.9806 - val_sparse_categorical_accuracy: 0.5226\n",
      "Epoch 2/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9697 - sparse_categorical_accuracy: 0.5307 - val_loss: 0.9630 - val_sparse_categorical_accuracy: 0.5312\n",
      "Epoch 3/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9549 - sparse_categorical_accuracy: 0.5387 - val_loss: 0.9488 - val_sparse_categorical_accuracy: 0.5421\n",
      "Epoch 4/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9476 - sparse_categorical_accuracy: 0.5423 - val_loss: 0.9415 - val_sparse_categorical_accuracy: 0.5446\n",
      "Epoch 5/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9428 - sparse_categorical_accuracy: 0.5455 - val_loss: 0.9402 - val_sparse_categorical_accuracy: 0.5449\n",
      "Epoch 6/10\n",
      "1605/1605 [==============================] - 5s 3ms/step - loss: 0.9384 - sparse_categorical_accuracy: 0.5478 - val_loss: 0.9370 - val_sparse_categorical_accuracy: 0.5463\n",
      "Epoch 7/10\n",
      "1605/1605 [==============================] - 5s 3ms/step - loss: 0.9350 - sparse_categorical_accuracy: 0.5493 - val_loss: 0.9327 - val_sparse_categorical_accuracy: 0.5486\n",
      "Epoch 8/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9322 - sparse_categorical_accuracy: 0.5510 - val_loss: 0.9314 - val_sparse_categorical_accuracy: 0.5507\n",
      "Epoch 9/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9301 - sparse_categorical_accuracy: 0.5523 - val_loss: 0.9304 - val_sparse_categorical_accuracy: 0.5505\n",
      "Epoch 10/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9283 - sparse_categorical_accuracy: 0.5537 - val_loss: 0.9317 - val_sparse_categorical_accuracy: 0.5493\n",
      "No. of classes: 5\n",
      "Epoch 1/10\n",
      "1605/1605 [==============================] - 5s 3ms/step - loss: 1.0234 - sparse_categorical_accuracy: 0.4961 - val_loss: 0.9897 - val_sparse_categorical_accuracy: 0.5156\n",
      "Epoch 2/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9789 - sparse_categorical_accuracy: 0.5226 - val_loss: 0.9763 - val_sparse_categorical_accuracy: 0.5259\n",
      "Epoch 3/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9654 - sparse_categorical_accuracy: 0.5305 - val_loss: 0.9622 - val_sparse_categorical_accuracy: 0.5337\n",
      "Epoch 4/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9512 - sparse_categorical_accuracy: 0.5389 - val_loss: 0.9530 - val_sparse_categorical_accuracy: 0.5374\n",
      "Epoch 5/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9422 - sparse_categorical_accuracy: 0.5460 - val_loss: 0.9462 - val_sparse_categorical_accuracy: 0.5432\n",
      "Epoch 6/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9375 - sparse_categorical_accuracy: 0.5476 - val_loss: 0.9414 - val_sparse_categorical_accuracy: 0.5445\n",
      "Epoch 7/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9349 - sparse_categorical_accuracy: 0.5493 - val_loss: 0.9430 - val_sparse_categorical_accuracy: 0.5415\n",
      "Epoch 8/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9327 - sparse_categorical_accuracy: 0.5505 - val_loss: 0.9375 - val_sparse_categorical_accuracy: 0.5476\n",
      "Epoch 9/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9308 - sparse_categorical_accuracy: 0.5514 - val_loss: 0.9355 - val_sparse_categorical_accuracy: 0.5494\n",
      "Epoch 10/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9287 - sparse_categorical_accuracy: 0.5534 - val_loss: 0.9362 - val_sparse_categorical_accuracy: 0.5479\n",
      "No. of classes: 5\n",
      "Epoch 1/10\n",
      "1605/1605 [==============================] - 5s 3ms/step - loss: 1.0193 - sparse_categorical_accuracy: 0.5001 - val_loss: 0.9889 - val_sparse_categorical_accuracy: 0.5139\n",
      "Epoch 2/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9784 - sparse_categorical_accuracy: 0.5228 - val_loss: 0.9721 - val_sparse_categorical_accuracy: 0.5263\n",
      "Epoch 3/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9636 - sparse_categorical_accuracy: 0.5310 - val_loss: 0.9539 - val_sparse_categorical_accuracy: 0.5373\n",
      "Epoch 4/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9518 - sparse_categorical_accuracy: 0.5389 - val_loss: 0.9443 - val_sparse_categorical_accuracy: 0.5428\n",
      "Epoch 5/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9451 - sparse_categorical_accuracy: 0.5424 - val_loss: 0.9398 - val_sparse_categorical_accuracy: 0.5458\n",
      "Epoch 6/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9400 - sparse_categorical_accuracy: 0.5457 - val_loss: 0.9332 - val_sparse_categorical_accuracy: 0.5480\n",
      "Epoch 7/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9363 - sparse_categorical_accuracy: 0.5470 - val_loss: 0.9326 - val_sparse_categorical_accuracy: 0.5473\n",
      "Epoch 8/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9335 - sparse_categorical_accuracy: 0.5494 - val_loss: 0.9295 - val_sparse_categorical_accuracy: 0.5515\n",
      "Epoch 9/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9311 - sparse_categorical_accuracy: 0.5514 - val_loss: 0.9265 - val_sparse_categorical_accuracy: 0.5530\n",
      "Epoch 10/10\n",
      "1605/1605 [==============================] - 4s 3ms/step - loss: 0.9292 - sparse_categorical_accuracy: 0.5507 - val_loss: 0.9258 - val_sparse_categorical_accuracy: 0.5522\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=FOLDS, test_size=0.3, random_state=RANDOM_SEED)\n",
    "sss.get_n_splits(X, y)\n",
    "\n",
    "kfold = 0\n",
    "results_df_arr = []\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train[num_features] = scaler.fit_transform(X_train[num_features])\n",
    "    X_test[num_features] = scaler.transform(X_test[num_features])\n",
    "    \n",
    "    model = create_MLP(X_train, y_train)\n",
    "    es_callback = EarlyStopping(monitor=\"val_sparse_categorical_accuracy\", min_delta=0, patience=patience)\n",
    "\n",
    "    checkpoint_filepath = os.path.join('../evaluation', 'any_day_comparisons', 'CLA_cp-epoch{epoch:02d}-loss{val_loss:.2f}.ckpt')\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "    \n",
    "    model.compile(loss=SparseCategoricalCrossentropy(), \n",
    "                optimizer=Adam(learning_rate=learning_rate), \n",
    "                metrics='sparse_categorical_accuracy')\n",
    "    \n",
    "    input_train_x = {}    \n",
    "    input_train_x['ohe_rid'] = X_train.filter(like='route_id_direction_')\n",
    "    input_train_x['ohe_holiday'] = X_train.filter(like='is_holiday_')\n",
    "    input_train_x['ohe_dow'] = X_train.filter(like='dayofweek_')\n",
    "    input_train_x['ohe_break'] = X_train.filter(like='is_school_break_')\n",
    "    input_train_x['ohe_tw'] = X_train.filter(like='time_window_')\n",
    "    input_train_x['numerical_input'] = X_train[num_features + [f\"{o}_ix\" for o in ord_features]]\n",
    "    \n",
    "    model.fit(x=input_train_x,\n",
    "              y=y_train,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[es_callback],\n",
    "              verbose=1)\n",
    "    \n",
    "    input_test_x = {}\n",
    "    input_test_x['ohe_rid'] = X_test.filter(like='route_id_direction_')\n",
    "    input_test_x['ohe_holiday'] = X_test.filter(like='is_holiday_')\n",
    "    input_test_x['ohe_dow'] = X_test.filter(like='dayofweek_')\n",
    "    input_test_x['ohe_break'] = X_test.filter(like='is_school_break_')\n",
    "    input_test_x['ohe_tw'] = X_test.filter(like='time_window_')\n",
    "    input_test_x['numerical_input'] = X_test[num_features + [f\"{o}_ix\" for o in ord_features]]\n",
    "\n",
    "    predictions = model.predict(x=input_test_x)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    res_df = original_rf.iloc[test_index]\n",
    "    res_df['y_pred'] = y_pred\n",
    "    res_df['y_true'] = y_test.to_numpy()\n",
    "    res_df['kfold'] = kfold\n",
    "    res_df = reconstruct_original_data_from_RF(res_df, ix_map, ohe_encoder)\n",
    "    kfold = kfold + 1\n",
    "    \n",
    "    results_df_arr.append(res_df)\n",
    "    K.clear_session()\n",
    "\n",
    "fp = os.path.join('../evaluation', 'any_day_comparisons', 'MLP_raw_results.pkl')\n",
    "pd.concat(results_df_arr).to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transit_date</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>traffic_speed</th>\n",
       "      <th>precipitation_intensity</th>\n",
       "      <th>scheduled_headway</th>\n",
       "      <th>y_reg100</th>\n",
       "      <th>y_class</th>\n",
       "      <th>time_window</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "      <th>kfold</th>\n",
       "      <th>route_id_direction</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_school_break</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144340</th>\n",
       "      <td>2021-01-18</td>\n",
       "      <td>231502</td>\n",
       "      <td>45.342601</td>\n",
       "      <td>0.537400</td>\n",
       "      <td>19.889300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5028.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5_TO DOWNTOWN</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348350</th>\n",
       "      <td>2022-01-22</td>\n",
       "      <td>263828</td>\n",
       "      <td>24.513094</td>\n",
       "      <td>0.678635</td>\n",
       "      <td>19.617309</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>4710.156250</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>56_TO DOWNTOWN</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221509</th>\n",
       "      <td>2021-06-24</td>\n",
       "      <td>242337</td>\n",
       "      <td>85.094709</td>\n",
       "      <td>0.544875</td>\n",
       "      <td>17.135672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1898.916667</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4_TO DOWNTOWN</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189121</th>\n",
       "      <td>2021-04-28</td>\n",
       "      <td>242197</td>\n",
       "      <td>75.056664</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>17.565668</td>\n",
       "      <td>0.098147</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29_TO DOWNTOWN</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293491</th>\n",
       "      <td>2021-10-23</td>\n",
       "      <td>260851</td>\n",
       "      <td>69.902532</td>\n",
       "      <td>0.554235</td>\n",
       "      <td>17.608281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7200.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28_TO DOWNTOWN</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       transit_date trip_id  temperature  humidity  traffic_speed  \\\n",
       "144340   2021-01-18  231502    45.342601  0.537400      19.889300   \n",
       "348350   2022-01-22  263828    24.513094  0.678635      19.617309   \n",
       "221509   2021-06-24  242337    85.094709  0.544875      17.135672   \n",
       "189121   2021-04-28  242197    75.056664  0.851000      17.565668   \n",
       "293491   2021-10-23  260851    69.902532  0.554235      17.608281   \n",
       "\n",
       "        precipitation_intensity  scheduled_headway  y_reg100  y_class  \\\n",
       "144340                 0.000000        5028.000000       7.0        0   \n",
       "348350                 0.002347        4710.156250      41.0        2   \n",
       "221509                 0.000000        1898.916667      13.0        1   \n",
       "189121                 0.098147        1800.000000      14.0        1   \n",
       "293491                 0.000000        7200.000000      10.0        1   \n",
       "\n",
       "       time_window  y_pred  y_true  kfold route_id_direction is_holiday  \\\n",
       "144340          37       0       0      0      5_TO DOWNTOWN       True   \n",
       "348350          22       2       2      0     56_TO DOWNTOWN      False   \n",
       "221509          22       0       1      0      4_TO DOWNTOWN      False   \n",
       "189121          34       0       1      0     29_TO DOWNTOWN      False   \n",
       "293491          33       0       1      0     28_TO DOWNTOWN      False   \n",
       "\n",
       "       dayofweek is_school_break  year  month  hour  day  \n",
       "144340         2           False  2021      1    18   18  \n",
       "348350         7           False  2022      1    11   22  \n",
       "221509         5            True  2021      6    11   24  \n",
       "189121         4           False  2021      4    17   28  \n",
       "293491         7           False  2021     10    16   23  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('../evaluation', 'any_day_comparisons', 'MLP_raw_results.pkl')\n",
    "mlp_results = pd.read_pickle(fp)\n",
    "mlp_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = deepcopy(df)\n",
    "tdf = triplevel_utils.generate_new_features(tdf, time_window=WINDOW, past_trips=PAST_TRIPS, target=TARGET)\n",
    "# Group by time windows and get the maximum of the aggregate load/class/sched\n",
    "# Get mean of temperature (mostly going to be equal)\n",
    "tdf = tdf.groupby(['transit_date', 'route_id_direction', 'time_window']).agg({\"arrival_time\":\"first\",\n",
    "                                                                              \"block_abbr\":\"first\",\n",
    "                                                                              \"trip_id\":\"first\",\n",
    "                                                                              \"year\":\"first\", \n",
    "                                                                              \"month\":\"first\",\n",
    "                                                                              \"day\": \"first\",\n",
    "                                                                              \"dayofweek\":\"first\", \n",
    "                                                                              \"hour\":\"first\",\n",
    "                                                                              \"is_holiday\": \"first\",\n",
    "                                                                              \"is_school_break\": \"first\",\n",
    "                                                                              \"temperature\":\"mean\", \n",
    "                                                                              \"humidity\":\"mean\",\n",
    "                                                                              \"precipitation_intensity\": \"mean\",\n",
    "                                                                              \"scheduled_headway\": \"max\",\n",
    "                                                                              TARGET: \"max\"})\n",
    "                                                                            #   \"traffic_speed\":\"mean\",\n",
    "tdf = tdf.reset_index(level=[0,1,2])\n",
    "tdf = tdf.sort_values(by=['block_abbr', 'arrival_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ohe_encoder is for the following column order: ['route_id_direction', 'is_holiday', 'dayofweek', 'is_school_break', 'time_window']\n"
     ]
    }
   ],
   "source": [
    "print(\"ohe_encoder is for the following column order:\", cat_features)\n",
    "\n",
    "rf_df, ix_map, ohe_encoder, percentiles = triplevel_utils.prepare_df_for_training(tdf, cat_features, ord_features, target=TARGET)\n",
    "rf_df, percentiles = triplevel_utils.adjust_bins(rf_df, TARGET=TARGET, percentiles=percentiles)\n",
    "\n",
    "original_rf = deepcopy(rf_df)\n",
    "original_rf['time_window'] = tdf['time_window']\n",
    "\n",
    "drop_cols = ['time_window', 'route_id', 'route_direction_name', 'block_abbr', 'y_reg100', 'y_reg095', 'is_holiday', 'route_id_direction', 'actual_headways', 'trip_id', 'arrival_time']\n",
    "drop_cols = [col for col in drop_cols if col in rf_df.columns]\n",
    "rf_df = rf_df.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dates = ('2020-01-01', '2021-09-30')\n",
    "val_dates =   ('2021-09-30', '2021-11-30')\n",
    "test_dates =  ('2021-11-30', '2022-04-06')\n",
    "\n",
    "train = rf_df[(rf_df['transit_date'] >= train_dates[0]) &\\\n",
    "            (rf_df['transit_date'] < train_dates[1])]\n",
    "val   = rf_df[(rf_df['transit_date'] >= val_dates[0]) &\\\n",
    "            (rf_df['transit_date'] < val_dates[1])]\n",
    "test  = rf_df[(rf_df['transit_date'] >= test_dates[0]) &\\\n",
    "            (rf_df['transit_date'] <= test_dates[1])]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train[num_features] = scaler.fit_transform(train[num_features])\n",
    "val[num_features] = scaler.transform(val[num_features])\n",
    "test[num_features] = scaler.transform(test[num_features])\n",
    "\n",
    "drop_cols = ['transit_date']\n",
    "train = train.drop(columns=drop_cols, axis=1)\n",
    "val = val.drop(columns=drop_cols, axis=1)\n",
    "test = test.drop(columns=drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def timeseries_dataset_from_dataset(df, label_slice, input_sequence_length, output_sequence_length, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(df.values)\n",
    "    ds = dataset.window(input_sequence_length + output_sequence_length, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x).batch(input_sequence_length + output_sequence_length)\n",
    "    def split_feature_label(x):\n",
    "        return x[:input_sequence_length], x[input_sequence_length:,label_slice]\n",
    "    ds = ds.map(split_feature_label)\n",
    "    return ds.batch(batch_size)\n",
    "\n",
    "num_of_classes = len(y.unique())\n",
    "def create_LSTM(n_lstm_units, n_timesteps, n_features, num_of_classes=num_of_classes):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(LSTM(n_lstm_units, input_shape=(n_timesteps, n_features), return_sequences=False))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(num_of_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label index: 5\n"
     ]
    }
   ],
   "source": [
    "input_sequence_length = 5\n",
    "output_sequence_length = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "patience = 3\n",
    "\n",
    "# label_slice = slice(5, 6, None)\n",
    "label_index = train.columns.tolist().index('y_class')\n",
    "print(\"Label index:\", label_index)\n",
    "label_slice = slice(label_index, label_index + 1, None) # which column the label/labels are\n",
    "\n",
    "dataset_train = timeseries_dataset_from_dataset(train, label_slice, \n",
    "                                                input_sequence_length=input_sequence_length,\n",
    "                                                output_sequence_length=output_sequence_length,\n",
    "                                                batch_size=batch_size)\n",
    "dataset_val  = timeseries_dataset_from_dataset(val, label_slice, \n",
    "                                                input_sequence_length=input_sequence_length,\n",
    "                                                output_sequence_length=output_sequence_length,\n",
    "                                                batch_size=batch_size)\n",
    "dataset_test  = timeseries_dataset_from_dataset(test, label_slice, \n",
    "                                                input_sequence_length=input_sequence_length,\n",
    "                                                output_sequence_length=output_sequence_length,\n",
    "                                                batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "     10/Unknown - 1s 6ms/step - loss: 1.4109 - sparse_categorical_accuracy: 0.3086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 15:26:33.904644: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089/1089 [==============================] - 12s 10ms/step - loss: 0.9061 - sparse_categorical_accuracy: 0.5732 - val_loss: 1.0239 - val_sparse_categorical_accuracy: 0.5255\n",
      "Epoch 2/20\n",
      "1089/1089 [==============================] - 10s 9ms/step - loss: 0.8239 - sparse_categorical_accuracy: 0.6227 - val_loss: 0.9649 - val_sparse_categorical_accuracy: 0.5647\n",
      "Epoch 3/20\n",
      "1089/1089 [==============================] - 8s 7ms/step - loss: 0.8057 - sparse_categorical_accuracy: 0.6318 - val_loss: 0.9397 - val_sparse_categorical_accuracy: 0.5794\n",
      "Epoch 4/20\n",
      "1089/1089 [==============================] - 8s 8ms/step - loss: 0.7962 - sparse_categorical_accuracy: 0.6370 - val_loss: 0.9197 - val_sparse_categorical_accuracy: 0.5876\n",
      "Epoch 5/20\n",
      "1089/1089 [==============================] - 10s 9ms/step - loss: 0.7899 - sparse_categorical_accuracy: 0.6401 - val_loss: 0.9045 - val_sparse_categorical_accuracy: 0.5943\n",
      "Epoch 6/20\n",
      "1089/1089 [==============================] - 10s 9ms/step - loss: 0.7854 - sparse_categorical_accuracy: 0.6425 - val_loss: 0.8936 - val_sparse_categorical_accuracy: 0.5973\n",
      "Epoch 7/20\n",
      "1089/1089 [==============================] - 9s 8ms/step - loss: 0.7816 - sparse_categorical_accuracy: 0.6446 - val_loss: 0.8853 - val_sparse_categorical_accuracy: 0.6002\n",
      "Epoch 8/20\n",
      "1089/1089 [==============================] - 10s 9ms/step - loss: 0.7779 - sparse_categorical_accuracy: 0.6467 - val_loss: 0.8792 - val_sparse_categorical_accuracy: 0.6026\n",
      "Epoch 9/20\n",
      "1089/1089 [==============================] - 10s 9ms/step - loss: 0.7745 - sparse_categorical_accuracy: 0.6487 - val_loss: 0.8760 - val_sparse_categorical_accuracy: 0.6042\n",
      "Epoch 10/20\n",
      "1089/1089 [==============================] - 9s 9ms/step - loss: 0.7714 - sparse_categorical_accuracy: 0.6505 - val_loss: 0.8754 - val_sparse_categorical_accuracy: 0.6029\n",
      "Epoch 1/20\n",
      "1089/1089 [==============================] - 11s 9ms/step - loss: 0.9032 - sparse_categorical_accuracy: 0.5719 - val_loss: 1.0227 - val_sparse_categorical_accuracy: 0.5384\n",
      "Epoch 2/20\n",
      "1089/1089 [==============================] - 9s 9ms/step - loss: 0.8205 - sparse_categorical_accuracy: 0.6238 - val_loss: 0.9648 - val_sparse_categorical_accuracy: 0.5697\n",
      "Epoch 3/20\n",
      "1089/1089 [==============================] - 9s 8ms/step - loss: 0.8031 - sparse_categorical_accuracy: 0.6333 - val_loss: 0.9411 - val_sparse_categorical_accuracy: 0.5817\n",
      "Epoch 4/20\n",
      "1089/1089 [==============================] - 10s 9ms/step - loss: 0.7931 - sparse_categorical_accuracy: 0.6384 - val_loss: 0.9247 - val_sparse_categorical_accuracy: 0.5876\n",
      "Epoch 5/20\n",
      "1089/1089 [==============================] - 8s 8ms/step - loss: 0.7866 - sparse_categorical_accuracy: 0.6420 - val_loss: 0.9133 - val_sparse_categorical_accuracy: 0.5916\n",
      "Epoch 6/20\n",
      "1089/1089 [==============================] - 9s 9ms/step - loss: 0.7814 - sparse_categorical_accuracy: 0.6445 - val_loss: 0.9022 - val_sparse_categorical_accuracy: 0.5972\n",
      "Epoch 7/20\n",
      "1089/1089 [==============================] - 8s 7ms/step - loss: 0.7767 - sparse_categorical_accuracy: 0.6469 - val_loss: 0.8909 - val_sparse_categorical_accuracy: 0.5999\n",
      "Epoch 8/20\n",
      "1089/1089 [==============================] - 9s 9ms/step - loss: 0.7723 - sparse_categorical_accuracy: 0.6487 - val_loss: 0.8843 - val_sparse_categorical_accuracy: 0.6025\n",
      "Epoch 9/20\n",
      "1089/1089 [==============================] - 9s 9ms/step - loss: 0.7681 - sparse_categorical_accuracy: 0.6513 - val_loss: 0.8811 - val_sparse_categorical_accuracy: 0.6040\n",
      "Epoch 10/20\n",
      "1089/1089 [==============================] - 8s 7ms/step - loss: 0.7641 - sparse_categorical_accuracy: 0.6538 - val_loss: 0.8813 - val_sparse_categorical_accuracy: 0.6014\n",
      "Epoch 11/20\n",
      "1089/1089 [==============================] - 8s 8ms/step - loss: 0.7602 - sparse_categorical_accuracy: 0.6554 - val_loss: 0.8844 - val_sparse_categorical_accuracy: 0.5966\n",
      "Epoch 1/20\n",
      "1089/1089 [==============================] - 11s 9ms/step - loss: 0.8896 - sparse_categorical_accuracy: 0.5816 - val_loss: 1.0066 - val_sparse_categorical_accuracy: 0.5448\n",
      "Epoch 2/20\n",
      "1089/1089 [==============================] - 8s 7ms/step - loss: 0.8210 - sparse_categorical_accuracy: 0.6239 - val_loss: 0.9743 - val_sparse_categorical_accuracy: 0.5674\n",
      "Epoch 3/20\n",
      "1089/1089 [==============================] - 10s 9ms/step - loss: 0.8028 - sparse_categorical_accuracy: 0.6336 - val_loss: 0.9563 - val_sparse_categorical_accuracy: 0.5779\n",
      "Epoch 4/20\n",
      "1089/1089 [==============================] - 9s 8ms/step - loss: 0.7926 - sparse_categorical_accuracy: 0.6387 - val_loss: 0.9311 - val_sparse_categorical_accuracy: 0.5863\n",
      "Epoch 5/20\n",
      "1089/1089 [==============================] - 8s 7ms/step - loss: 0.7861 - sparse_categorical_accuracy: 0.6425 - val_loss: 0.9093 - val_sparse_categorical_accuracy: 0.5931\n",
      "Epoch 6/20\n",
      "1089/1089 [==============================] - 9s 8ms/step - loss: 0.7806 - sparse_categorical_accuracy: 0.6452 - val_loss: 0.8937 - val_sparse_categorical_accuracy: 0.5976\n",
      "Epoch 7/20\n",
      "1089/1089 [==============================] - 10s 9ms/step - loss: 0.7754 - sparse_categorical_accuracy: 0.6478 - val_loss: 0.8861 - val_sparse_categorical_accuracy: 0.6009\n",
      "Epoch 8/20\n",
      "1089/1089 [==============================] - 8s 8ms/step - loss: 0.7701 - sparse_categorical_accuracy: 0.6504 - val_loss: 0.8825 - val_sparse_categorical_accuracy: 0.6034\n",
      "Epoch 9/20\n",
      "1089/1089 [==============================] - 7s 7ms/step - loss: 0.7654 - sparse_categorical_accuracy: 0.6524 - val_loss: 0.8783 - val_sparse_categorical_accuracy: 0.6028\n",
      "Epoch 10/20\n",
      "1089/1089 [==============================] - 9s 8ms/step - loss: 0.7605 - sparse_categorical_accuracy: 0.6548 - val_loss: 0.8780 - val_sparse_categorical_accuracy: 0.6037\n",
      "Epoch 11/20\n",
      "1089/1089 [==============================] - 9s 8ms/step - loss: 0.7548 - sparse_categorical_accuracy: 0.6576 - val_loss: 0.8818 - val_sparse_categorical_accuracy: 0.6011\n",
      "Epoch 12/20\n",
      "1089/1089 [==============================] - 8s 7ms/step - loss: 0.7488 - sparse_categorical_accuracy: 0.6612 - val_loss: 0.8898 - val_sparse_categorical_accuracy: 0.5952\n",
      "Epoch 13/20\n",
      "1089/1089 [==============================] - 10s 9ms/step - loss: 0.7416 - sparse_categorical_accuracy: 0.6650 - val_loss: 0.9005 - val_sparse_categorical_accuracy: 0.5885\n"
     ]
    }
   ],
   "source": [
    "results_df_arr = []\n",
    "for n_lstm_units in [64, 128, 256]:\n",
    "    number_of_features = len(train.columns)\n",
    "    model = create_LSTM(n_lstm_units, input_sequence_length, number_of_features)\n",
    "\n",
    "    model.compile(loss=SparseCategoricalCrossentropy(), \n",
    "                    optimizer=Adam(learning_rate=learning_rate), \n",
    "                    metrics=SparseCategoricalAccuracy())\n",
    "\n",
    "    es_callback = EarlyStopping(monitor=\"val_sparse_categorical_accuracy\",\n",
    "                                min_delta=0.1, patience=patience)\n",
    "    model.fit(dataset_train,\n",
    "              epochs=epochs,\n",
    "              validation_data=dataset_val,\n",
    "              callbacks=[es_callback],\n",
    "              verbose=1)\n",
    "\n",
    "    predictions = model.predict(dataset_test)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    res_df = original_rf.loc[test.index][5:]\n",
    "    res_df['y_pred'] = y_pred\n",
    "    res_df['y_true'] = test[input_sequence_length:]['y_class'].to_numpy()\n",
    "    res_df['n_lstm_units'] = n_lstm_units\n",
    "    res_df = reconstruct_original_data_from_RF(res_df, ix_map, ohe_encoder)\n",
    "    \n",
    "    results_df_arr.append(res_df)\n",
    "    # model.reset_states()\n",
    "    K.clear_session()\n",
    "\n",
    "fp = os.path.join('../evaluation', 'any_day_comparisons', 'LSTM_raw_results.pkl')\n",
    "pd.concat(results_df_arr).to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151533, 23)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('../evaluation', 'any_day_comparisons', 'LSTM_raw_results.pkl')\n",
    "lstm_results = pd.read_pickle(fp)\n",
    "lstm_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637594157aac4ac4ab885a40cd4c8c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/430404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73def33da0f46bd8e56698b9cd27585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/143468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562b0cd1686449bdb3fe28482c69fb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/143468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dfb0363d1f4da68741124d506cdbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/143468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_statistical_prediction(row, df, percentile, lookback_duration, TARGET='y_reg100'):\n",
    "    trip_id = row.trip_id\n",
    "    transit_date = row.transit_date\n",
    "    route_id_direction = row.route_id_direction\n",
    "    lookback_date = transit_date - pd.Timedelta(lookback_duration)\n",
    "    tdf = df[(df['transit_date'] >= lookback_date) & \\\n",
    "             (df['transit_date'] < transit_date)]\n",
    "    tdf = tdf[(tdf['trip_id'] == trip_id) & \\\n",
    "              (tdf['route_id_direction'] == route_id_direction)]\n",
    "    if tdf.empty:\n",
    "        return -1\n",
    "    return np.percentile(tdf[TARGET].to_numpy(), percentile)\n",
    "\n",
    "processed_triplevel = os.path.join('../data', 'processed', 'triplevel_df.parquet')\n",
    "df = pd.read_parquet(processed_triplevel, engine='auto')\n",
    "df = df.dropna()\n",
    "df = df.drop(['time_window', 'load'], axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.sort_values(['block_abbr', 'transit_date', 'arrival_time', 'route_id_direction'])\n",
    "\n",
    "percentiles = [(0.0, 9.0), (10.0, 16.0), (17.0, 55.0), (56.0, 75.0), (76.0, 100.0)]\n",
    "df['y_class'] = df[TARGET].swifter.apply(lambda x: data_utils.get_class(x, percentiles))\n",
    "df['y_class'] = df['y_class'].astype('int')\n",
    "\n",
    "df['minute'] = df['arrival_time'].dt.minute\n",
    "df['minuteByWindow'] = df['minute'] // WINDOW\n",
    "df['temp'] = df['minuteByWindow'] + (df['hour'] * 60 / WINDOW)\n",
    "df['time_window'] = np.floor(df['temp']).astype('int')\n",
    "df = df.drop(columns=['minute', 'minuteByWindow', 'temp'], axis=1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=FOLDS, random_state=RANDOM_SEED, shuffle=True)\n",
    "X, y = df[['transit_date', 'trip_id', 'arrival_time', 'route_id_direction', 'time_window', TARGET]], df['y_class']\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "lookback_distances = ['4W', '2W', '1W']\n",
    "percentile = 1.0\n",
    "results_df_arr = []\n",
    "for _, test_index in skf.split(X, y):\n",
    "    for lookback_distance in lookback_distances:\n",
    "        baseline_X = X.iloc[test_index]\n",
    "        baseline_Y = y.iloc[test_index]\n",
    "        \n",
    "        y_pred = baseline_X.swifter.apply(lambda x: get_statistical_prediction(x, df, percentile, lookback_distance, TARGET=TARGET), axis=1)\n",
    "        y_true = baseline_Y.to_numpy()\n",
    "        # res_df = deepcopy(X.loc[test_index])\n",
    "        res_df = deepcopy(baseline_X)\n",
    "        res_df['y_pred'] = y_pred.to_numpy()\n",
    "        res_df['y_true'] = y_true\n",
    "        # res_df = reconstruct_original_data_from_RF(res_df, ix_map, ohe_encoder)\n",
    "        results_df_arr.append(res_df)\n",
    "    break\n",
    "\n",
    "results_df_arr[0]['y_pred_class'] = results_df_arr[0]['y_pred'].apply(lambda x: data_utils.get_class(x, percentiles))\n",
    "results_df_arr[1]['y_pred_class'] = results_df_arr[1]['y_pred'].apply(lambda x: data_utils.get_class(x, percentiles))\n",
    "results_df_arr[2]['y_pred_class'] = results_df_arr[2]['y_pred'].apply(lambda x: data_utils.get_class(x, percentiles))\n",
    "df1 = results_df_arr[0].dropna(subset=['y_pred_class'])\n",
    "df2 = results_df_arr[1].dropna(subset=['y_pred_class'])\n",
    "df3 = results_df_arr[2].dropna(subset=['y_pred_class'])\n",
    "df1 = df1.rename(columns={'y_pred_class': 'y_pred'})\n",
    "df2 = df2.rename(columns={'y_pred_class': 'y_pred'})\n",
    "df3 = df3.rename(columns={'y_pred_class': 'y_pred'})\n",
    "df1['past'] = 1\n",
    "df2['past'] = 2\n",
    "df3['past'] = 4\n",
    "\n",
    "fp = os.path.join('../evaluation', 'any_day_comparisons', 'BASELINE_raw_results.pkl')\n",
    "pd.concat([df1, df2, df3]).to_pickle(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transit_date</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>route_id_direction</th>\n",
       "      <th>time_window</th>\n",
       "      <th>y_reg100</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>past</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301336</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>195743</td>\n",
       "      <td>2020-01-03 07:03:16</td>\n",
       "      <td>3_FROM DOWNTOWN</td>\n",
       "      <td>14</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44946</th>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>195729</td>\n",
       "      <td>2020-01-14 13:06:16</td>\n",
       "      <td>3_FROM DOWNTOWN</td>\n",
       "      <td>26</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241070</th>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>195766</td>\n",
       "      <td>2020-01-14 13:48:36</td>\n",
       "      <td>3_TO DOWNTOWN</td>\n",
       "      <td>27</td>\n",
       "      <td>24.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44947</th>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>195762</td>\n",
       "      <td>2020-01-14 15:19:38</td>\n",
       "      <td>3_TO DOWNTOWN</td>\n",
       "      <td>30</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90135</th>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>195757</td>\n",
       "      <td>2020-01-14 16:45:39</td>\n",
       "      <td>3_TO DOWNTOWN</td>\n",
       "      <td>33</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       transit_date trip_id        arrival_time route_id_direction  \\\n",
       "301336   2020-01-03  195743 2020-01-03 07:03:16    3_FROM DOWNTOWN   \n",
       "44946    2020-01-14  195729 2020-01-14 13:06:16    3_FROM DOWNTOWN   \n",
       "241070   2020-01-14  195766 2020-01-14 13:48:36      3_TO DOWNTOWN   \n",
       "44947    2020-01-14  195762 2020-01-14 15:19:38      3_TO DOWNTOWN   \n",
       "90135    2020-01-14  195757 2020-01-14 16:45:39      3_TO DOWNTOWN   \n",
       "\n",
       "        time_window  y_reg100  y_pred  y_true  y_pred  past  \n",
       "301336           14      18.0    14.0       2     1.0     1  \n",
       "44946            26      24.0    20.0       2     2.0     1  \n",
       "241070           27      24.0    15.0       2     1.0     1  \n",
       "44947            30      49.0    33.0       2     2.0     1  \n",
       "90135            33      19.0    23.0       2     2.0     1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('../evaluation', 'any_day_comparisons', 'BASELINE_raw_results.pkl')\n",
    "baseline_results = pd.read_pickle(fp)\n",
    "baseline_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for plotting results, see `plotting_paper.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transit_date</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>route_id_direction</th>\n",
       "      <th>time_window</th>\n",
       "      <th>y_reg100</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286072</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>195844</td>\n",
       "      <td>2020-01-01 11:50:51</td>\n",
       "      <td>3_FROM DOWNTOWN</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407007</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>195717</td>\n",
       "      <td>2020-01-02 18:04:02</td>\n",
       "      <td>3_FROM DOWNTOWN</td>\n",
       "      <td>36</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301336</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>195743</td>\n",
       "      <td>2020-01-03 07:03:16</td>\n",
       "      <td>3_FROM DOWNTOWN</td>\n",
       "      <td>14</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>195778</td>\n",
       "      <td>2020-01-03 07:41:40</td>\n",
       "      <td>3_TO DOWNTOWN</td>\n",
       "      <td>15</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135589</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>195775</td>\n",
       "      <td>2020-01-03 09:28:04</td>\n",
       "      <td>3_TO DOWNTOWN</td>\n",
       "      <td>18</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145294</th>\n",
       "      <td>2021-07-30</td>\n",
       "      <td>246346</td>\n",
       "      <td>2021-07-30 22:54:48</td>\n",
       "      <td>50_TO DOWNTOWN</td>\n",
       "      <td>45</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206042</th>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>246346</td>\n",
       "      <td>2021-08-23 22:54:40</td>\n",
       "      <td>50_TO DOWNTOWN</td>\n",
       "      <td>45</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296678</th>\n",
       "      <td>2021-08-26</td>\n",
       "      <td>246346</td>\n",
       "      <td>2021-08-26 22:43:42</td>\n",
       "      <td>50_TO DOWNTOWN</td>\n",
       "      <td>45</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251619</th>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>246345</td>\n",
       "      <td>2021-09-01 23:34:34</td>\n",
       "      <td>50_FROM DOWNTOWN</td>\n",
       "      <td>47</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86244</th>\n",
       "      <td>2021-09-29</td>\n",
       "      <td>246345</td>\n",
       "      <td>2021-09-29 23:33:44</td>\n",
       "      <td>50_FROM DOWNTOWN</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143468 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       transit_date trip_id        arrival_time route_id_direction  \\\n",
       "286072   2020-01-01  195844 2020-01-01 11:50:51    3_FROM DOWNTOWN   \n",
       "407007   2020-01-02  195717 2020-01-02 18:04:02    3_FROM DOWNTOWN   \n",
       "301336   2020-01-03  195743 2020-01-03 07:03:16    3_FROM DOWNTOWN   \n",
       "40       2020-01-03  195778 2020-01-03 07:41:40      3_TO DOWNTOWN   \n",
       "135589   2020-01-03  195775 2020-01-03 09:28:04      3_TO DOWNTOWN   \n",
       "...             ...     ...                 ...                ...   \n",
       "145294   2021-07-30  246346 2021-07-30 22:54:48     50_TO DOWNTOWN   \n",
       "206042   2021-08-23  246346 2021-08-23 22:54:40     50_TO DOWNTOWN   \n",
       "296678   2021-08-26  246346 2021-08-26 22:43:42     50_TO DOWNTOWN   \n",
       "251619   2021-09-01  246345 2021-09-01 23:34:34   50_FROM DOWNTOWN   \n",
       "86244    2021-09-29  246345 2021-09-29 23:33:44   50_FROM DOWNTOWN   \n",
       "\n",
       "        time_window  y_reg100  y_pred  y_true  \n",
       "286072           23       3.0   -1.00       0  \n",
       "407007           36       9.0   -1.00       0  \n",
       "301336           14      18.0   14.00       2  \n",
       "40               15      13.0   -1.00       1  \n",
       "135589           18      21.0   -1.00       2  \n",
       "...             ...       ...     ...     ...  \n",
       "145294           45      13.0    5.16       1  \n",
       "206042           45      15.0    6.06       1  \n",
       "296678           45      12.0    6.06       1  \n",
       "251619           47       2.0    2.04       0  \n",
       "86244            47       0.0    0.08       0  \n",
       "\n",
       "[143468 rows x 8 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
