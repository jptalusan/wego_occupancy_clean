{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"/media/seconddrive/mta_stationing_problem\")\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/17 23:49:07 WARN Utils: Your hostname, scope-vanderbilt resolves to a loopback address: 127.0.1.1; using 10.2.218.69 instead (on interface enp8s0)\n",
      "22/08/17 23:49:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/17 23:49:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/17 23:49:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "#         .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '80g').master(\"local[26]\")\\\n",
    "#         .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "#         .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "#         .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "#         .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "#         .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "#         .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\\\n",
    "#         .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\\\n",
    "#         .config(\"spark.ui.showConsoleProgress\", \"false\")\\\n",
    "#         .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "                .config(\"spark.sql.session.timeZone\", \"UTC\")\\\n",
    "                .config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "                .config('spark.driver.memory', '2g')\\\n",
    "                .config('spark.executor.memory', '2g')\\\n",
    "                .config(\"spark.ui.showConsoleProgress\", \"false\")\\\n",
    "                .appName(\"app\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging apc data to service disruption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get APC data\n",
    "f = os.path.join('data', 'processed', 'apc_weather_gtfs.parquet')\n",
    "apcdata = spark.read.load(f)\n",
    "get_columns = ['zero_load_at_trip_end', 'stop_sequence', 'stop_id_original', 'stop_name']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "# # filter subset\n",
    "query = f\"\"\"\n",
    "SELECT {get_str}\n",
    "FROM apc\n",
    "ORDER BY arrival_time\n",
    "\"\"\"\n",
    "apcdata = spark.sql(query)\n",
    "# apcdata = apcdata.filter(apcdata.zero_load_at_trip_end == 1)\n",
    "apcdata = apcdata.dropDuplicates(['stop_id_original'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apcdata.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zero_load_at_trip_end</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_id_original</th>\n",
       "      <th>stop_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>100OAKS</td>\n",
       "      <td>100 OAKS MALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21WE</td>\n",
       "      <td>21ST AVE PAST WEST END AVE SB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>25THYDNM</td>\n",
       "      <td>25TH AVE N &amp; HYDE ST NORTHBOUND MID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>26ACLANN</td>\n",
       "      <td>26TH AVE N &amp; CLARKSVILLE PIKE NB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7AVHARNN</td>\n",
       "      <td>7TH &amp; HARRSION NB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ANDMCAWN</td>\n",
       "      <td>ANDERSON LN &amp; MCARTHUR DR WB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>ARTS</td>\n",
       "      <td>ARTS MAGNET SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>BILTAPTS</td>\n",
       "      <td>BILTMORE APARTMENTS AT GLASTONBURY &amp; MASSMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>BNA</td>\n",
       "      <td>NASHVILLE INTERNATIONAL AIRPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>BOYBUENN</td>\n",
       "      <td>BOYD DR &amp; BUENAVIEW BLVD NB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>CHI23AWF</td>\n",
       "      <td>CHILDRENS WAY &amp; 23RD AVE WB FARSIDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>CLK E11</td>\n",
       "      <td>CLARKSVILLE EXIT 11 P&amp;R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>CLSFCLK</td>\n",
       "      <td>CLIFF DR PAST CLARKSVILLE HWY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DICBELSN</td>\n",
       "      <td>DICKERSON PK &amp; BELL GRIMES LN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>EWIBRIWN</td>\n",
       "      <td>EWING DR &amp; BRICK CHURCH PIKE WB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>EZECONWN</td>\n",
       "      <td>EZELL PIKE CONVENIENCE CTR WB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>GWYEWIN</td>\n",
       "      <td>GWYNNWOOD DR &amp; EWING DR NB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>HICHICNN</td>\n",
       "      <td>HICKORY HOLLOW PL &amp; HICKORY HOLLOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>JOHASHEN</td>\n",
       "      <td>JOHN MALLETTE DR &amp; ASHTON AVE EB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>LPSCI</td>\n",
       "      <td>LIPSCOMB UNIVERSITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_13</td>\n",
       "      <td>CENTRAL 4TH AVE - BAY 13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_14</td>\n",
       "      <td>CENTRAL 4TH AVE - BAY 14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_15</td>\n",
       "      <td>CENTRAL 4TH AVE - BAY 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_16</td>\n",
       "      <td>CENTRAL 4TH AVE - BAY 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_18</td>\n",
       "      <td>CENTRAL 4TH AVE - BAY 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_20</td>\n",
       "      <td>CENTRAL 4TH AVE - BAY 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_22</td>\n",
       "      <td>CENTRAL 4TH AVE - BAY 22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_23</td>\n",
       "      <td>CENTRAL 4TH AVE - BAY 23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC5_10</td>\n",
       "      <td>CENTRAL 5TH AVE - BAY 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC5_11</td>\n",
       "      <td>CENTRAL 5TH AVE - BAY 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC5_12</td>\n",
       "      <td>CENTRAL 5TH AVE - BAY 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC5_9</td>\n",
       "      <td>CENTRAL 5TH AVE - BAY 9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>OLDCENNF</td>\n",
       "      <td>OLD HICKORY BLVD &amp; CENTRAL PIKE NB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>OLDSONSN</td>\n",
       "      <td>OLD HICKORY BLVD &amp; SONYA DR SB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>SMRY</td>\n",
       "      <td>SMYRNA STARS &amp; STRIKES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>SPFD1</td>\n",
       "      <td>SPRINGFIELD D/T P &amp; R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>STEBLAS</td>\n",
       "      <td>STEWARTS FERRY PKWY &amp; BLACKWOOD DR SB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>TEN51S</td>\n",
       "      <td>TENNESSEE AVE &amp; 51ST AVE N EB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>WCHS</td>\n",
       "      <td>WHITES CREEK HIGH SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>WED16ASN</td>\n",
       "      <td>WEDGEWOOD AVE &amp; 16TH AVE WB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>WHICHASF</td>\n",
       "      <td>WHITE BRIDGE PIKE &amp; CHARLOTTE SHONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      zero_load_at_trip_end  stop_sequence stop_id_original  \\\n",
       "1272                    1.0              1          100OAKS   \n",
       "667                     0.0              1             21WE   \n",
       "1326                    NaN              1         25THYDNM   \n",
       "33                      NaN              1         26ACLANN   \n",
       "1355                    1.0              1         7AVHARNN   \n",
       "722                     NaN              1         ANDMCAWN   \n",
       "82                      NaN              1             ARTS   \n",
       "99                      NaN              1         BILTAPTS   \n",
       "755                     NaN              1              BNA   \n",
       "102                     NaN              1         BOYBUENN   \n",
       "800                     NaN              1         CHI23AWF   \n",
       "1499                    NaN              1          CLK E11   \n",
       "810                     NaN              1          CLSFCLK   \n",
       "1539                    0.0              1         DICBELSN   \n",
       "893                     NaN              1         EWIBRIWN   \n",
       "1625                    NaN              1         EZECONWN   \n",
       "1665                    NaN              1          GWYEWIN   \n",
       "1721                    NaN              1         HICHICNN   \n",
       "370                     NaN              1         JOHASHEN   \n",
       "414                     NaN              1            LPSCI   \n",
       "422                     NaN              1          MCC4_13   \n",
       "1034                    NaN              1          MCC4_14   \n",
       "1035                    NaN              1          MCC4_15   \n",
       "1036                    NaN              1          MCC4_16   \n",
       "1823                    0.0              1          MCC4_18   \n",
       "1824                    NaN              1          MCC4_20   \n",
       "1825                    NaN              1          MCC4_22   \n",
       "424                     1.0              1          MCC4_23   \n",
       "1826                    NaN              1          MCC5_10   \n",
       "425                     NaN              1          MCC5_11   \n",
       "1827                    NaN              1          MCC5_12   \n",
       "1041                    0.0              1           MCC5_9   \n",
       "508                     1.0              1         OLDCENNF   \n",
       "1942                    NaN              1         OLDSONSN   \n",
       "2006                    NaN              1             SMRY   \n",
       "2009                    NaN              1            SPFD1   \n",
       "2011                    NaN              1          STEBLAS   \n",
       "2017                    NaN              1           TEN51S   \n",
       "602                     NaN              1             WCHS   \n",
       "2049                    NaN              1         WED16ASN   \n",
       "2083                    NaN              1         WHICHASF   \n",
       "\n",
       "                                         stop_name  \n",
       "1272                                 100 OAKS MALL  \n",
       "667                  21ST AVE PAST WEST END AVE SB  \n",
       "1326           25TH AVE N & HYDE ST NORTHBOUND MID  \n",
       "33                26TH AVE N & CLARKSVILLE PIKE NB  \n",
       "1355                             7TH & HARRSION NB  \n",
       "722                   ANDERSON LN & MCARTHUR DR WB  \n",
       "82                              ARTS MAGNET SCHOOL  \n",
       "99    BILTMORE APARTMENTS AT GLASTONBURY & MASSMAN  \n",
       "755                NASHVILLE INTERNATIONAL AIRPORT  \n",
       "102                    BOYD DR & BUENAVIEW BLVD NB  \n",
       "800            CHILDRENS WAY & 23RD AVE WB FARSIDE  \n",
       "1499                       CLARKSVILLE EXIT 11 P&R  \n",
       "810                  CLIFF DR PAST CLARKSVILLE HWY  \n",
       "1539                 DICKERSON PK & BELL GRIMES LN  \n",
       "893                EWING DR & BRICK CHURCH PIKE WB  \n",
       "1625                 EZELL PIKE CONVENIENCE CTR WB  \n",
       "1665                    GWYNNWOOD DR & EWING DR NB  \n",
       "1721            HICKORY HOLLOW PL & HICKORY HOLLOW  \n",
       "370               JOHN MALLETTE DR & ASHTON AVE EB  \n",
       "414                            LIPSCOMB UNIVERSITY  \n",
       "422                       CENTRAL 4TH AVE - BAY 13  \n",
       "1034                      CENTRAL 4TH AVE - BAY 14  \n",
       "1035                      CENTRAL 4TH AVE - BAY 15  \n",
       "1036                      CENTRAL 4TH AVE - BAY 16  \n",
       "1823                      CENTRAL 4TH AVE - BAY 18  \n",
       "1824                      CENTRAL 4TH AVE - BAY 20  \n",
       "1825                      CENTRAL 4TH AVE - BAY 22  \n",
       "424                       CENTRAL 4TH AVE - BAY 23  \n",
       "1826                      CENTRAL 5TH AVE - BAY 10  \n",
       "425                       CENTRAL 5TH AVE - BAY 11  \n",
       "1827                      CENTRAL 5TH AVE - BAY 12  \n",
       "1041                       CENTRAL 5TH AVE - BAY 9  \n",
       "508             OLD HICKORY BLVD & CENTRAL PIKE NB  \n",
       "1942                OLD HICKORY BLVD & SONYA DR SB  \n",
       "2006                        SMYRNA STARS & STRIKES  \n",
       "2009                         SPRINGFIELD D/T P & R  \n",
       "2011         STEWARTS FERRY PKWY & BLACKWOOD DR SB  \n",
       "2017                 TENNESSEE AVE & 51ST AVE N EB  \n",
       "602                       WHITES CREEK HIGH SCHOOL  \n",
       "2049                   WEDGEWOOD AVE & 16TH AVE WB  \n",
       "2083           WHITE BRIDGE PIKE & CHARLOTTE SHONE  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['stop_sequence'] == 1].sort_values(by=['stop_id_original'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"/media/seconddrive/mta_stationing_problem\")\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "#         .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '80g').master(\"local[26]\")\\\n",
    "#         .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "#         .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "#         .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "#         .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "#         .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "#         .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\\\n",
    "#         .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\\\n",
    "#         .config(\"spark.ui.showConsoleProgress\", \"false\")\\\n",
    "#         .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "                .config(\"spark.sql.session.timeZone\", \"UTC\")\\\n",
    "                .config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "                .config('spark.driver.memory', '2g')\\\n",
    "                .config('spark.executor.memory', '2g')\\\n",
    "                .config(\"spark.ui.showConsoleProgress\", \"false\")\\\n",
    "                .appName(\"app\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging apc data to service disruption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APC_START = '2020-01-01'\n",
    "APC_END   = '2022-04-06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get APC data\n",
    "f = os.path.join('data', 'processed', 'apc_weather_gtfs.parquet')\n",
    "apcdata = spark.read.load(f)\n",
    "todelete = apcdata.filter('(load < 0) OR (load IS NULL)').select('transit_date','trip_id','overload_id').distinct()\n",
    "todelete=todelete.withColumn('marker',F.lit(1))\n",
    "apcdataafternegdelete=apcdata.join(todelete,on=['trip_id','transit_date','overload_id'],how='left').filter('marker is null').drop('marker')\n",
    "apcdataafternegdelete = apcdataafternegdelete.sort(['trip_id', 'overload_id'])\n",
    "get_columns = ['trip_id', 'transit_date', 'arrival_time', 'arrival_time_str', 'vehicle_id',\n",
    "               'block_abbr', 'stop_sequence', 'stop_id_original', 'overload_id']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "apcdataafternegdelete.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "# # filter subset\n",
    "query = f\"\"\"\n",
    "SELECT {get_str}\n",
    "FROM apc\n",
    "ORDER BY arrival_time\n",
    "\"\"\"\n",
    "apcdata = spark.sql(query)\n",
    "apcdata = apcdata.dropna(subset=['arrival_time'])\n",
    "apcdata = apcdata.na.fill(value=-1)\n",
    "apcdata = apcdata.orderBy(\"arrival_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apcdata.groupBy().agg(F.count(F.when(F.col(\"overload_id\")>0, True))).collect()[0][0]\n",
    "# apcdata.groupBy().agg(F.count(F.when(F.col(\"overload_id\")>0, True))).show()\n",
    "# +------------------------------------------------+\n",
    "# |count(CASE WHEN (overload_id > 0) THEN true END)|\n",
    "# +------------------------------------------------+\n",
    "# |                                           14371|\n",
    "# +------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apcdata.groupBy('overload_id').count().show()\n",
    "# +-----------+--------+\n",
    "# |overload_id|   count|\n",
    "# +-----------+--------+\n",
    "# |          1|    9295|\n",
    "# |          4|      47|\n",
    "# |          2|    4702|\n",
    "# |          0|15792788|\n",
    "# |          3|     327|\n",
    "# +-----------+--------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get service disruption dataset\n",
    "fp = os.path.join('data', 'others', 'Service Disruptions_07_2019_08_2022.csv')\n",
    "disruptions_df = pd.read_csv(fp)\n",
    "disruptions_df.head()\n",
    "disruptions_df['DATETIME'] = disruptions_df['DATE'] + ' ' + disruptions_df['TIME']\n",
    "disruptions_df['DATE'] = pd.to_datetime(disruptions_df['DATE'], format='%m/%d/%y', errors='coerce')\n",
    "disruptions_df['TIME'] = pd.to_datetime(disruptions_df['TIME'], format='%H:%M:%S', errors='coerce')\n",
    "disruptions_df['DATETIME'] = pd.to_datetime(disruptions_df['DATETIME'], format='%m/%d/%y %H:%M:%S', errors='coerce')\n",
    "disruptions_df = disruptions_df[(disruptions_df['DATE'] >= APC_START) & (disruptions_df['DATE'] <= APC_END)]\n",
    "\n",
    "# Remove weather related disruptions\n",
    "disruptions_df = disruptions_df[(disruptions_df['REASON'] != 'Weather')].sort_values(by=['DATETIME']).reset_index(drop=True)\n",
    "print('Shape:', disruptions_df.shape)\n",
    "disruptions_df = disruptions_df.drop(columns=['COMMENTS'])\n",
    "disruptions_df['BLOCK'] = disruptions_df['BLOCK'].astype('int32')\n",
    "\n",
    "# Convert to spark dataframe for merging\n",
    "disruptions_sp = spark.createDataFrame(disruptions_df)\n",
    "disruptions_sp = disruptions_sp.withColumn(\"BLOCK\", F.col(\"BLOCK\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disruptions_sp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apcdata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asof_join_backward(l, r):\n",
    "    return pd.merge_asof(l, r, left_on='DATETIME', right_on='arrival_time', direction=\"backward\", left_by=\"BLOCK\", right_by=\"block_abbr\", tolerance=pd.Timedelta('30d'))\n",
    "\n",
    "identified_disruptions_backward = disruptions_sp.groupby(\"BLOCK\").cogroup(apcdata.groupby(\"block_abbr\")).applyInPandas(\n",
    "                                asof_join_backward, schema=\"\"\"DATE timestamp, TIME timestamp, BLOCK int, REASON string, START_STOP_ABBR string, \n",
    "                                                     START_STOP_NAME string, START_STOP_LATITUDE double, START_STOP_LONGITUDE double, DATETIME timestamp,\n",
    "                                                     trip_id string, transit_date timestamp, arrival_time timestamp, arrival_time_str string, vehicle_id string,\n",
    "                                                     block_abbr int, stop_sequence int, stop_id_original string, overload_id int\"\"\")\n",
    "\n",
    "def asof_join_forward(l, r):\n",
    "    return pd.merge_asof(l, r, left_on='DATETIME', right_on='arrival_time', direction=\"forward\", left_by=\"BLOCK\", right_by=\"block_abbr\", tolerance=pd.Timedelta('30d'))\n",
    "\n",
    "identified_disruptions_forward = disruptions_sp.groupby(\"BLOCK\").cogroup(apcdata.groupby(\"block_abbr\")).applyInPandas(\n",
    "                                asof_join_forward, schema=\"\"\"DATE timestamp, TIME timestamp, BLOCK int, REASON string, START_STOP_ABBR string, \n",
    "                                                     START_STOP_NAME string, START_STOP_LATITUDE double, START_STOP_LONGITUDE double, DATETIME timestamp,\n",
    "                                                     trip_id string, transit_date timestamp, arrival_time timestamp, arrival_time_str string, vehicle_id string,\n",
    "                                                     block_abbr int, stop_sequence int, stop_id_original string, overload_id int\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_disruptions_backward_df = identified_disruptions_backward.toPandas()\n",
    "identified_disruptions_forward_df  = identified_disruptions_forward.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['DATETIME', 'BLOCK', 'REASON', 'START_STOP_ABBR', \n",
    "        'trip_id', 'transit_date', 'vehicle_id', 'block_abbr', 'arrival_time', 'stop_sequence', 'stop_id_original', 'overload_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_disruptions_backward_df = identified_disruptions_backward_df[cols]\n",
    "identified_disruptions_forward_df = identified_disruptions_forward_df[cols]\n",
    "\n",
    "identified_disruptions_df = identified_disruptions_backward_df.merge(identified_disruptions_forward_df, \n",
    "                                                                     how='outer', \n",
    "                                                                     on=['DATETIME', 'BLOCK', 'REASON', 'START_STOP_ABBR'], \n",
    "                                                                     suffixes=('_prev', '_next'))\n",
    "identified_disruptions_df = identified_disruptions_df.dropna(subset=['trip_id_prev', 'trip_id_next'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_disruptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think many service disruptions here occur at the ends of their trips therefore they have no effect on the succeeding trips.  \n",
    "No need to send an overload bus since another bus in service can be used to cover? (does this happen?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = os.path.join('data', 'processed', 'matched_service_disruptions.pkl')\n",
    "# identified_disruptions_df.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'processed', 'matched_service_disruptions.pkl')\n",
    "identified_disruptions_df = pd.read_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_disruptions_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out events where the past stop, next stop and reported disruption all happened on different days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_same_day = identified_disruptions_df[(identified_disruptions_df['DATETIME'].dt.date != identified_disruptions_df['transit_date_prev']) & \n",
    "                          (identified_disruptions_df['DATETIME'].dt.date != identified_disruptions_df['transit_date_next']) & \n",
    "                          (identified_disruptions_df['transit_date_prev'] != identified_disruptions_df['transit_date_next'])].sort_values(by=['DATETIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_same_day[not_same_day['vehicle_id_prev'] == not_same_day['vehicle_id_next']].head()\n",
    "display(not_same_day.head(1))\n",
    "not_same_day.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking specific disruptions\n",
    "def show_slice(disruptions_df, idx):\n",
    "    disruption_idx = idx\n",
    "    display(disruptions_df.loc[[disruption_idx]])\n",
    "    tdf = disruptions_df.loc[disruption_idx]\n",
    "    disruption_datetime = tdf['DATETIME']\n",
    "    start_time = tdf['arrival_time_prev'] - pd.Timedelta('7h')\n",
    "    end_time   = tdf['arrival_time_next'] + pd.Timedelta('7h')\n",
    "    block      = int(tdf['BLOCK'])\n",
    "    if pd.isnull(start_time):\n",
    "        start_time = disruption_datetime\n",
    "    if pd.isnull(end_time):\n",
    "        end_time = disruption_datetime\n",
    "    print(start_time, end_time)\n",
    "    specific_apcdata = apcdata.filter(F.col(\"arrival_time\").between(start_time, end_time))\n",
    "    specific_apcdata = specific_apcdata.where(specific_apcdata.\tblock_abbr == block)\n",
    "    return specific_apcdata.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Just load all overload data in APC to memory and work on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overload_apc = apcdata.where(F.col(\"overload_id\") > 0)\n",
    "overload_df = overload_apc.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "pbar = tqdm(total=len(not_same_day))\n",
    "for k, tdf in not_same_day.iterrows():\n",
    "    disruption_datetime = tdf['DATETIME']\n",
    "    start_time = tdf['arrival_time_prev'] - pd.Timedelta('1h')\n",
    "    end_time   = tdf['arrival_time_next'] + pd.Timedelta('1h')\n",
    "    block      = int(tdf['BLOCK'])\n",
    "    tdf = overload_df[(overload_df['block_abbr'] == block)]\n",
    "    tdf = tdf[(tdf['arrival_time'] >= start_time) & (tdf['arrival_time'] <= end_time)]\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(f\"Processing {k}:{len(tdf[tdf['overload_id'] > 0])}\")\n",
    "    results[k] = len(tdf)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disruption reported and vehicle was changed between service days (counted as overload)\n",
    "data = {'id':list(results.keys()), 'overloads':list(results.values())}\n",
    "tdf = pd.DataFrame(data)\n",
    "tdf[tdf['overloads'] > 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disruption reported but either no APC data or no trip for that same block for the vehicle until a few days after\n",
    "tdf[tdf['overloads'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = show_slice(not_same_day, 174)\n",
    "with pd.option_context(\"display.max_rows\", 100):\n",
    "    display(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_day = identified_disruptions_df[(identified_disruptions_df['DATETIME'].dt.date == identified_disruptions_df['transit_date_prev']) & \n",
    "                                     (identified_disruptions_df['DATETIME'].dt.date == identified_disruptions_df['transit_date_next'])]\n",
    "same_day.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "pbar = tqdm(total=len(same_day))\n",
    "for k, tdf in same_day.iterrows():\n",
    "    disruption_datetime = tdf['DATETIME']\n",
    "    start_time = tdf['arrival_time_prev'] - pd.Timedelta('7h')\n",
    "    end_time   = tdf['arrival_time_next'] + pd.Timedelta('7h')\n",
    "    block      = int(tdf['BLOCK'])\n",
    "    tdf = overload_df[(overload_df['block_abbr'] == block)]\n",
    "    tdf = tdf[(tdf['arrival_time'] >= start_time) & (tdf['arrival_time'] <= end_time)]\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(f\"Processing {k}:{len(tdf[tdf['overload_id'] > 0])}\")\n",
    "    results[k] = len(tdf)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disruption reported and an overload bus was dispatched (our use case)\n",
    "data = {'id':list(results.keys()), 'overloads':list(results.values())}\n",
    "tdf = pd.DataFrame(data)\n",
    "tdf[tdf['overloads'] > 0]\n",
    "# tdf[tdf['overloads'] > 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disruption reported but no overload bus needed\n",
    "tdf[tdf['overloads'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'type':['same_day', 'same_day', 'not_same', 'not_same'], 'overload_status': ['used', 'not used', 'used', 'not_used'], 'count': [75, 604, 13, 2185]}\n",
    "dd = pd.DataFrame(d)\n",
    "# dd.plot.bar(stacked=True)\n",
    "dd.groupby('overload_status').sum().plot(kind='pie', y='count', autopct='%1.0f%%',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overload ID types\n",
    "* 0: Service bus\n",
    "* 1: Dispatched for emergency [9295]\n",
    "* 2: [4702]\n",
    "* 3: I think used when vehicle is still not able to ply the service\n",
    "* 4: Used in one trip only `230326` `2020-09-26`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_disruptions_df.loc[603]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = show_slice(same_day, 603)\n",
    "with pd.option_context(\"display.max_rows\", 100):\n",
    "    display(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify how many overload buses were dispatched in the middle of a trip vs just from a depot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = same_day.loc[tdf[tdf['overloads'] > 0].id][(same_day['stop_sequence_prev'] == 1) |\n",
    "                                           (same_day['stop_sequence_next'] == 1)]\n",
    "same_day.loc[tdf[tdf['overloads'] > 0].id].drop(a.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.pyplot as plt\n",
    "ax = same_day.loc[tdf[tdf['overloads'] > 0].id]['REASON'].value_counts().plot(kind='bar', rot=45)\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xticks(ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_day.loc[tdf[tdf['overloads'] > 0].id][(same_day['vehicle_id_prev'] != same_day['vehicle_id_next'])].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the APC data for the trip(s) above. It started the trip but due to disruptions, had to be covered by another vehicle.  \n",
    "Is vehicle `1913` a special overload vehicle?\n",
    "* answer is no, so certain in service vehicles can be used to cover for other vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_id = '245007'\n",
    "date    = '2021-06-10'\n",
    "block   = '5605'\n",
    "specific_apcdata = apcdata.where(F.col(\"transit_date\") == date)\n",
    "specific_apcdata = specific_apcdata.where(specific_apcdata.trip_id == trip_id)\n",
    "specific_apcdata = specific_apcdata.where(specific_apcdata.block_abbr == block)\n",
    "sdf = specific_apcdata.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    display(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking cases where apc data was initially not null and then null records started showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"/media/seconddrive/mta_stationing_problem\")\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "                .config(\"spark.sql.session.timeZone\", \"UTC\")\\\n",
    "                .config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "                .config('spark.driver.memory', '2g')\\\n",
    "                .config('spark.executor.memory', '2g')\\\n",
    "                .config(\"spark.ui.showConsoleProgress\", \"false\")\\\n",
    "                .appName(\"app\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'processed', 'matched_service_disruptions.pkl')\n",
    "identified_disruptions_df = pd.read_pickle(fp)\n",
    "identified_disruptions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_same_day = identified_disruptions_df[(identified_disruptions_df['DATETIME'].dt.date != identified_disruptions_df['transit_date_prev']) & \n",
    "                          (identified_disruptions_df['DATETIME'].dt.date != identified_disruptions_df['transit_date_next']) & \n",
    "                          (identified_disruptions_df['transit_date_prev'] != identified_disruptions_df['transit_date_next'])].sort_values(by=['DATETIME'])\n",
    "# not_same_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_day = identified_disruptions_df[(identified_disruptions_df['DATETIME'].dt.date == identified_disruptions_df['transit_date_prev']) & \n",
    "                                     (identified_disruptions_df['DATETIME'].dt.date == identified_disruptions_df['transit_date_next'])]\n",
    "# same_day[['DATETIME', 'BLOCK', 'trip_id_prev', 'trip_id_next']]\n",
    "same_day.sort_values(by=['DATETIME']).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_day[same_day['trip_id_next'] == same_day['trip_id_prev']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get APC data\n",
    "f = os.path.join(\"data\", \"apc\", \"cleaned-wego-daily.apc.parquet\")\n",
    "# f = os.path.join('data', 'processed', 'apc_weather_gtfs.parquet')\n",
    "apcdata = spark.read.load(f)\n",
    "get_columns = ['trip_id', 'transit_date', 'arrival_time', 'arrival_time_str', 'vehicle_id',\n",
    "               'block_abbr', 'stop_sequence', 'stop_id_original', 'overload_id']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "# # filter subset\n",
    "query = f\"\"\"\n",
    "SELECT {get_str}\n",
    "FROM apc\n",
    "ORDER BY arrival_time\n",
    "\"\"\"\n",
    "apcdata = spark.sql(query)\n",
    "apcdata = apcdata.orderBy(\"arrival_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trip_id = '245007'\n",
    "# block   = 5605\t\n",
    "# datetime = '2021-06-10 20:15:00'\n",
    "# date    = datetime.split(' ')[0]\n",
    "\n",
    "# specific_apcdata = apcdata.where(F.col(\"transit_date\") == date)\n",
    "# specific_apcdata = specific_apcdata.where(specific_apcdata.block_abbr == block)\n",
    "# sdf = specific_apcdata.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip_ids = sdf.groupby(['trip_id']).agg('last').sort_values('arrival_time')\n",
    "# trip_ids = trip_ids[trip_ids['arrival_time'] >= datetime].index.tolist()\n",
    "# new_sdf = []\n",
    "# for trip_id in trip_ids:\n",
    "#     new_sdf.append(sdf[sdf['trip_id'] == trip_id].sort_values(by=['stop_sequence']))\n",
    "# new_sdf = pd.concat(new_sdf).reset_index(drop=True)\n",
    "\n",
    "# with pd.option_context(\"display.max_rows\", 5):\n",
    "#     display(new_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdf_status(datetime, block):\n",
    "    date = datetime.split(' ')[0]\n",
    "\n",
    "    specific_apcdata = apcdata.where(F.col(\"transit_date\") == date)\n",
    "    specific_apcdata = specific_apcdata.where(specific_apcdata.block_abbr == block)\n",
    "    sdf = specific_apcdata.toPandas()\n",
    "    trip_ids = sdf.groupby(['trip_id']).agg('last').sort_values('arrival_time')\n",
    "    # Include next trip in case its the overload\n",
    "    trip_ids = trip_ids[trip_ids['arrival_time'] >= datetime].index.tolist()[0:2]\n",
    "    new_sdf = []    \n",
    "    for trip_id in trip_ids:\n",
    "        # Drop first and last stops (depots)\n",
    "        new_sdf.append(sdf[sdf['trip_id'] == trip_id].sort_values(by=['stop_sequence']).iloc[2:-2, :])\n",
    "        \n",
    "    if len(new_sdf) == 0:\n",
    "        return -1\n",
    "    \n",
    "    new_sdf = pd.concat(new_sdf).reset_index(drop=True)\n",
    "    new_sdf['arrival_time_str'].fillna(value='NULL', inplace=True)\n",
    "    new_sdf['is_null'] = pd.isnull(new_sdf['arrival_time'])\n",
    "\n",
    "    sametrip_overload_multiplier = 1\n",
    "    if len(new_sdf[(new_sdf['trip_id'] == trip_ids[0])]['overload_id'].unique()) > 1:\n",
    "        sametrip_overload_multiplier = 10\n",
    "    \n",
    "    nexttrip_overload_multiplier = 1\n",
    "    if len(trip_ids) > 1:\n",
    "        if len(new_sdf[(new_sdf['trip_id'] == trip_ids[1])]['overload_id'].unique()) > 1:\n",
    "            nexttrip_overload_multiplier = 10\n",
    "        \n",
    "    with pd.option_context(\"display.max_rows\", None):\n",
    "        display(new_sdf)\n",
    "        # display(new_sdf[(new_sdf['trip_id'] == trip_ids[0])])\n",
    "        # display(new_sdf[(new_sdf['trip_id'] == trip_ids[0]) & (new_sdf['overload_id'] == 0)])\n",
    "        \n",
    "    # Work on initial trip only and no overload\n",
    "    new_sdf = new_sdf[(new_sdf['trip_id'] == trip_ids[0]) & (new_sdf['overload_id'] == 0)]\n",
    "\n",
    "    all_valid = (new_sdf['is_null'] == False).all()\n",
    "    all_null = (new_sdf['is_null'] == True).all()\n",
    "    \n",
    "    first_null = False\n",
    "    mid_null = False\n",
    "    latest_null = False\n",
    "    null_groupby = new_sdf.groupby([(new_sdf.is_null != new_sdf.is_null.shift()).cumsum()])\n",
    "    ilen = len(null_groupby)\n",
    "    for i, g in null_groupby:\n",
    "        if i == 1:\n",
    "            first_null = g['is_null'].values[0]\n",
    "        if i > 1 and i < ilen and not mid_null:\n",
    "            mid_null = g['is_null'].values[0]\n",
    "        if i == ilen:\n",
    "            latest_null = g['is_null'].values[0]\n",
    "    # print(all_null, first_null, mid_null, latest_null, overload_multiplier)\n",
    "    \n",
    "    if all_valid:\n",
    "        return 1 * nexttrip_overload_multiplier * sametrip_overload_multiplier\n",
    "    if all_null:\n",
    "        return 2 * nexttrip_overload_multiplier * sametrip_overload_multiplier\n",
    "    if latest_null:\n",
    "        return 3 * nexttrip_overload_multiplier * sametrip_overload_multiplier\n",
    "    if not first_null and mid_null and not latest_null:\n",
    "        return 4 * nexttrip_overload_multiplier * sametrip_overload_multiplier\n",
    "    if first_null and not mid_null and not latest_null:\n",
    "        return 5 * nexttrip_overload_multiplier * sametrip_overload_multiplier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "# sd = same_day[same_day['trip_id_next'] == same_day['trip_id_prev']]\n",
    "sd = same_day\n",
    "sd['status'] = sd.swifter.apply(lambda x: get_sdf_status(x['DATETIME'].strftime('%Y-%m-%d %H:%M:%S'), x['BLOCK']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = os.path.join('data', 'processed', 'same_day_same_trip_status.pkl')\n",
    "# sd.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'processed', 'same_day_same_trip_status.pkl')\n",
    "sd = pd.read_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = []\n",
    "# sd = same_day[same_day['trip_id_next'] != same_day['trip_id_prev']]\n",
    "# sd = sd[sd['status'] == -1]\n",
    "# for k, row in sd[0:1].iterrows():\n",
    "for k, row in same_day.loc[[458]].iterrows():\n",
    "    block = row['BLOCK']\n",
    "    datetime = row['DATETIME'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    status = get_sdf_status(datetime, block)\n",
    "    print(datetime, k, status)\n",
    "    statuses.append(status)\n",
    "    # break\n",
    "statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'processed', 'same_day_same_trip_status.pkl')\n",
    "sd = pd.read_pickle(fp)\n",
    "\n",
    "sd.loc[sd[(sd['status'] == 1) | (sd['status'] == 10) | (sd['status'] == 100)].index, 'merged_status'] = 1\n",
    "sd.loc[sd[(sd['status'] == 2) | (sd['status'] == 20) | (sd['status'] == 200)].index, 'merged_status'] = 2\n",
    "sd.loc[sd[(sd['status'] == 3) | (sd['status'] == 30) | (sd['status'] == 300)].index, 'merged_status'] = 3\n",
    "sd.loc[sd[(sd['status'] == 4) | (sd['status'] == 40) | (sd['status'] == 400)].index, 'merged_status'] = 4\n",
    "sd.loc[sd[(sd['status'] == 5) | (sd['status'] == 50) | (sd['status'] == 500)].index, 'merged_status'] = 5\n",
    "sd.loc[sd[sd['status'].isna()].index, 'merged_status'] = 5\n",
    "sd.loc[sd[sd['status'].isna()].index, 'status'] = 5\n",
    "\n",
    "sd.loc[sd[(sd['status'] == 1)].index, 'type'] = \"no_overload\"\n",
    "sd.loc[sd[(sd['status'] == 2)].index, 'type'] = \"no_overload\"\n",
    "sd.loc[sd[(sd['status'] == 3)].index, 'type'] = \"no_overload\"\n",
    "sd.loc[sd[(sd['status'] == 4)].index, 'type'] = \"no_overload\"\n",
    "sd.loc[sd[(sd['status'] == 5)].index, 'type'] = \"no_overload\"\n",
    "\n",
    "sd.loc[sd[(sd['status'] == 10)].index, 'type'] = \"nexttrip_overload\"\n",
    "sd.loc[sd[(sd['status'] == 20)].index, 'type'] = \"nexttrip_overload\"\n",
    "sd.loc[sd[(sd['status'] == 30)].index, 'type'] = \"nexttrip_overload\"\n",
    "sd.loc[sd[(sd['status'] == 40)].index, 'type'] = \"nexttrip_overload\"\n",
    "sd.loc[sd[(sd['status'] == 50)].index, 'type'] = \"nexttrip_overload\"\n",
    "\n",
    "sd.loc[sd[(sd['status'] == 100)].index, 'type'] = \"sametrip_overload\"\n",
    "sd.loc[sd[(sd['status'] == 200)].index, 'type'] = \"sametrip_overload\"\n",
    "sd.loc[sd[(sd['status'] == 300)].index, 'type'] = \"sametrip_overload\"\n",
    "sd.loc[sd[(sd['status'] == 400)].index, 'type'] = \"sametrip_overload\"\n",
    "sd.loc[sd[(sd['status'] == 500)].index, 'type'] = \"sametrip_overload\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd[sd['type'] != 'no_overload']\n",
    "df_pivot = pd.pivot_table(sd[sd['type'] != 'no_overload'], values='status', index='REASON', columns='type',\n",
    "                          aggfunc='count').fillna(0).sort_values('nexttrip_overload', ascending=False)#[['no_overload', 'sametrip_overload', 'nexttrip_overload']]\n",
    "display(df_pivot)\n",
    "ax =df_pivot.plot(kind='bar', rot=45, stacked=True)\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xticks(ha='right')\n",
    "# ax.set_xticklabels(['no null', 'all null', 'null end', 'null mid', 'null start'])\n",
    "ax.set_xlabel('Trip status')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Handling disruptions')\n",
    "\n",
    "fp = os.path.join('plots', 'handling_disruptions_REASONS.png')\n",
    "plt.savefig(fp, dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = pd.pivot_table(sd, values='status', index='merged_status', columns='type',\n",
    "                          aggfunc='count').fillna(0)[['no_overload', 'sametrip_overload', 'nexttrip_overload']].sort_values('no_overload', ascending=False)\n",
    "\n",
    "ax = df_pivot.plot(kind='bar', rot=0, stacked=True)\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "labels = ['no null', 'all null', 'null end', 'null mid', 'null start']\n",
    "ax.set_xticklabels([labels[int(i) - 1] for i in df_pivot.index])\n",
    "ax.set_xlabel('Trip status')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Handling disruptions')\n",
    "\n",
    "# plt.xticks(ha='right')\n",
    "fp = os.path.join('plots', 'handling_disruptions.png')\n",
    "plt.savefig(fp, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling not same day data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "sd = not_same_day\n",
    "sd['status'] = sd.swifter.apply(lambda x: get_sdf_status(x['DATETIME'].strftime('%Y-%m-%d %H:%M:%S'), x['BLOCK']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = os.path.join('data', 'processed', 'not_same_day_same_trip_status.pkl')\n",
    "# sd.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'processed', 'not_same_day_same_trip_status.pkl')\n",
    "nsd = pd.read_pickle(fp)\n",
    "# nsd = nsd[nsd['status'] != -1]\n",
    "\n",
    "fp = os.path.join('data', 'processed', 'same_day_same_trip_status.pkl')\n",
    "sd = pd.read_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'processed', 'matched_service_disruptions.pkl')\n",
    "identified_disruptions_df = pd.read_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_disruptions = pd.concat([sd, nsd]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_disruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 1) | (all_disruptions['status'] == 10) | (all_disruptions['status'] == 100)].index, 'merged_status'] = 1\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 2) | (all_disruptions['status'] == 20) | (all_disruptions['status'] == 200)].index, 'merged_status'] = 2\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 3) | (all_disruptions['status'] == 30) | (all_disruptions['status'] == 300)].index, 'merged_status'] = 3\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 4) | (all_disruptions['status'] == 40) | (all_disruptions['status'] == 400)].index, 'merged_status'] = 4\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 5) | (all_disruptions['status'] == 50) | (all_disruptions['status'] == 500)].index, 'merged_status'] = 5\n",
    "all_disruptions.loc[all_disruptions[all_disruptions['status'].isna()].index, 'merged_status'] = 5\n",
    "all_disruptions.loc[all_disruptions[all_disruptions['status'].isna()].index, 'status'] = 5\n",
    "\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 1)].index, 'type'] = \"no_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 2)].index, 'type'] = \"no_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 3)].index, 'type'] = \"no_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 4)].index, 'type'] = \"no_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 5)].index, 'type'] = \"no_overload\"\n",
    "\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 10)].index, 'type'] = \"nexttrip_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 20)].index, 'type'] = \"nexttrip_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 30)].index, 'type'] = \"nexttrip_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 40)].index, 'type'] = \"nexttrip_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 50)].index, 'type'] = \"nexttrip_overload\"\n",
    "\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 100)].index, 'type'] = \"sametrip_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 200)].index, 'type'] = \"sametrip_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 300)].index, 'type'] = \"sametrip_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 400)].index, 'type'] = \"sametrip_overload\"\n",
    "all_disruptions.loc[all_disruptions[(all_disruptions['status'] == 500)].index, 'type'] = \"sametrip_overload\"\n",
    "all_disruptions.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = pd.pivot_table(all_disruptions, values='status', index='merged_status', columns='type',\n",
    "                          aggfunc='count').fillna(0)[['no_overload', 'sametrip_overload', 'nexttrip_overload']].sort_values('no_overload', ascending=False)\n",
    "\n",
    "ax = df_pivot.plot(kind='bar', rot=0, stacked=True, color=['#1f77b4', '#ff7f0e',  '#2ca02c'])\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "labels = ['no null', 'all null', 'null end', 'null mid', 'null start']\n",
    "ax.set_xticklabels([labels[int(i) - 1] for i in df_pivot.index])\n",
    "ax.set_xlabel('Trip status')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Handling disruptions')\n",
    "\n",
    "# plt.xticks(ha='right')\n",
    "fp = os.path.join('plots', 'nsd_handling_disruptions.png')\n",
    "plt.savefig(fp, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = pd.pivot_table(all_disruptions[all_disruptions['type'] != 'no_overload'], values='status', index='REASON', columns='type',\n",
    "                          aggfunc='count').fillna(0).sort_values('nexttrip_overload', ascending=False)#[['no_overload', 'sametrip_overload', 'nexttrip_overload']]\n",
    "display(df_pivot)\n",
    "ax = df_pivot.plot(kind='bar', rot=45, stacked=True, color=['#2ca02c', '#ff7f0e'])\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xticks(ha='right')\n",
    "# ax.set_xticklabels(['no null', 'all null', 'null end', 'null mid', 'null start'])\n",
    "ax.set_xlabel('Trip status')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Handling disruptions')\n",
    "\n",
    "fp = os.path.join('plots', 'handling_disruptions_REASONS.png')\n",
    "plt.savefig(fp, dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding school schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_breaks = [#2019-2020\n",
    "                 ('2019-10-07', '2019-10-11'),\n",
    "                 ('2019-11-27', '2019-11-29'),\n",
    "                 ('2019-12-20', '2020-01-06'),\n",
    "                 ('2020-03-16', '2020-03-20'),\n",
    "                 ('2020-05-22', '2020-08-04'),\n",
    "                 # 2020-2021\n",
    "                 ('2020-10-02', '2020-10-09'),\n",
    "                 ('2020-11-25', '2020-11-27'),\n",
    "                 ('2020-12-18', '2021-01-04'),\n",
    "                 ('2021-03-15', '2021-03-19'),\n",
    "                 ('2021-05-26', '2021-08-09'),\n",
    "                 # 2022-2023\n",
    "                 ('2021-09-02', '2021-09-05'),\n",
    "                 ('2021-10-07', '2021-10-14'),\n",
    "                 ('2021-11-23', '2021-11-25'),\n",
    "                 ('2021-12-19', '2022-01-06'),\n",
    "                 ('2022-03-13', '2022-03-17'),\n",
    "                #  ('2022-05-25', '2022-08-09'),\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
